{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd16f0a-6c26-4fed-a0ec-3cf54b1736dd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and merge results #\n",
    "#############################\n",
    "\n",
    "# If your files are Excel:\n",
    "gemma_df = pd.read_excel(\"Gemma_Resultss.xlsx\")  # <-- adjust sheet name\n",
    "gpt_df   = pd.read_excel(\"Results_GPTa.xlsx\")    # <-- adjust sheet name\n",
    "\n",
    "# Or if you exported to CSV:\n",
    "#gemma_df = pd.read_csv(\"Gemma_Results.csv\")\n",
    "#gpt_df   = pd.read_csv(\"Results_GPT.csv\")\n",
    "\n",
    "# Add model labels (if not already present)\n",
    "gemma_df[\"model\"] = \"gemma\"\n",
    "gpt_df[\"model\"]   = \"gpt3.5\"\n",
    "\n",
    "# Make sure both have the same schema; keep only needed columns\n",
    "cols_to_keep = [\n",
    "    \"item_id\",\n",
    "    \"prompt\",\n",
    "    \"model\",\n",
    "    \"readability\",        # K–12 readability metric\n",
    "    \"morph_complexity\",   # your morphological complexity metric\n",
    "    \"clarity\",            # human rating dims\n",
    "    \"answer_acc\",\n",
    "    \"distractor_quality\",\n",
    "    \"word_diff_align\",\n",
    "    \"task_diff_align\",\n",
    "]\n",
    "\n",
    "df = pd.concat([gemma_df[cols_to_keep], gpt_df[cols_to_keep]], ignore_index=True)\n",
    "\n",
    "##########################################\n",
    "# 2. Descriptive stats by model/prompt   #\n",
    "##########################################\n",
    "\n",
    "metric_cols = [\n",
    "    \"readability\",\n",
    "    \"morph_complexity\",\n",
    "    \"clarity\",\n",
    "    \"answer_acc\",\n",
    "    \"distractor_quality\",\n",
    "    \"word_diff_align\",\n",
    "    \"task_diff_align\",\n",
    "]\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "\n",
    "summary = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Descriptive stats by model x prompt:\")\n",
    "print(summary)\n",
    "\n",
    "####################################################\n",
    "# 3. Reliability: Cronbach’s alpha for human dims  #\n",
    "####################################################\n",
    "\n",
    "def cronbach_alpha(df_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    df_scores: DataFrame of shape (n_items, n_dims)\n",
    "    rows = items; columns = rating dimensions (clarity, etc.)\n",
    "    \"\"\"\n",
    "    # Drop rows with all NaNs\n",
    "    X = df_scores.dropna(how=\"all\").values\n",
    "    k = X.shape[1]  # number of dimensions\n",
    "\n",
    "    if k < 2:\n",
    "        return np.nan\n",
    "\n",
    "    # variance of each item (dimension)\n",
    "    var_items = X.var(axis=0, ddof=1)\n",
    "    # variance of total score\n",
    "    total_scores = X.sum(axis=1)\n",
    "    var_total = total_scores.var(ddof=1)\n",
    "\n",
    "    alpha = (k / (k - 1)) * (1 - var_items.sum() / var_total)\n",
    "    return alpha\n",
    "\n",
    "human_dims = [\n",
    "    \"clarity\",\n",
    "    \"answer_acc\",\n",
    "    \"distractor_quality\",\n",
    "    \"word_diff_align\",\n",
    "    \"task_diff_align\",\n",
    "]\n",
    "\n",
    "# Overall alpha across all items/models\n",
    "alpha_overall = cronbach_alpha(df[human_dims])\n",
    "print(f\"\\nCronbach's alpha (all items, all models): {alpha_overall:.3f}\")\n",
    "\n",
    "# Optional: alpha per model\n",
    "for m in df[\"model\"].unique():\n",
    "    alpha_m = cronbach_alpha(df.loc[df[\"model\"] == m, human_dims])\n",
    "    print(f\"Cronbach's alpha for {m}: {alpha_m:.3f}\")\n",
    "\n",
    "###########################################################\n",
    "# 4. Significance: Gemma vs GPT-3.5 (paired by item+prompt)\n",
    "###########################################################\n",
    "\n",
    "# We assume: for each (item_id, prompt) we have one Gemma row and one GPT row.\n",
    "\n",
    "def paired_t_tests(df, metric, model_col=\"model\",\n",
    "                   id_cols=(\"item_id\", \"prompt\"),\n",
    "                   model_a=\"gemma\", model_b=\"gpt3.5\"):\n",
    "    \"\"\"\n",
    "    Paired t-tests between model_a and model_b on 'metric',\n",
    "    pairing on id_cols (e.g., (item_id, prompt)).\n",
    "    Returns a DataFrame of t-stat, p-value, and effect size per prompt.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # do test separately for each prompt (more interpretable)\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        # pivot: index = item_id (or item+prompt), columns = model, values = metric\n",
    "        wide = (\n",
    "            sub.pivot_table(index=list(id_cols), columns=model_col, values=metric)\n",
    "               .dropna(subset=[model_a, model_b], how=\"any\")\n",
    "        )\n",
    "\n",
    "        if wide.shape[0] < 3:\n",
    "            # too few pairs for reliable t-test\n",
    "            continue\n",
    "\n",
    "        x = wide[model_a].values\n",
    "        y = wide[model_b].values\n",
    "\n",
    "        t_stat, p_val = stats.ttest_rel(x, y)\n",
    "\n",
    "        # Cohen's d (paired)\n",
    "        diff = x - y\n",
    "        d = diff.mean() / diff.std(ddof=1)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_pairs\": wide.shape[0],\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": d,\n",
    "                \"mean_\" + model_a: x.mean(),\n",
    "                \"mean_\" + model_b: y.mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Example: significance tests for K–12-relevant metrics\n",
    "test_metrics = [\"readability\", \"morph_complexity\",\n",
    "                \"clarity\", \"answer_acc\",\n",
    "                \"distractor_quality\", \"word_diff_align\",\n",
    "                \"task_diff_align\"]\n",
    "\n",
    "all_tests = []\n",
    "for m in test_metrics:\n",
    "    res = paired_t_tests(df, metric=m)\n",
    "    all_tests.append(res)\n",
    "\n",
    "all_tests_df = pd.concat(all_tests, ignore_index=True)\n",
    "\n",
    "print(\"\\nPaired t-tests (Gemma vs GPT-3.5) by prompt & metric:\")\n",
    "print(all_tests_df.sort_values([\"metric\", \"prompt\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8e529-e06e-4a48-943e-e988b1754548",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# If your files are Excel:\n",
    "#  - Gemma sheet name: \"Gemma Results\"\n",
    "#  - GPT sheet name:   \"Results\"\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "# Expected columns (based on what you wrote):\n",
    "# Gemma: Prompting_strategy, Question_Type, ..., Overall_Score, Grammar_Score, Complexity_Score,\n",
    "#        Readability_Score, Fluency_Score, Error_Count, Errors, ...\n",
    "# GPT:   Row, Prompting_strategy, Question_Type, ..., Overall_Score, Grammar_Score, Complexity_Score,\n",
    "#        Readability_Score, Fluency_Score, Error_Count, Errors\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Common columns we care about\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset (this will raise a KeyError if any column is missing, so it's a good check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize column names: Prompting_strategy -> prompt\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Also add lowercase metric aliases for convenience\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "gemma = gemma.assign(**{new: gemma[old] for old, new in rename_metrics.items()})\n",
    "gpt   = gpt.assign(**{new: gpt[old]   for old, new in rename_metrics.items()})\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean up types (just to be safe)\n",
    "metric_cols = list(rename_metrics.values())\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "# Descriptives: mean, std, count by model and prompting strategy\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns: e.g. ('overall','mean') -> 'overall_mean'\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            # Not enough data for a reasonable t-test\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        # Cohen's d (pooled SD)\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Optional: per-prompt independent t-tests (robust version)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Welch t-test for each prompt separately, for one metric.\"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Require at least 2 items per group to be meaningful\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run for each metric and concatenate\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5):\")\n",
    "    print(per_prompt_tests.sort_values([\"metric\", \"prompt\"]))\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###########################################\n",
    "# 7. Save outputs (optional, for your paper)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3ad230-fb53-4060-bb13-829c8a1c361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 2. Standardize / rename columns     #\n",
    "#######################################\n",
    "\n",
    "# Gemma columns:\n",
    "# Prompting_strategy, Question_Type, Question, Correct_Answer, Choice_1, Choice_2, Choice_3,\n",
    "# Word_Difficulty, Task_Difficulty, Text, Overall_Score, Grammar_Score, Complexity_Score,\n",
    "# Readability_Score, Fluency_Score, Error_Count, Errors, sum of scores\n",
    "\n",
    "gemma_rename = {\n",
    "    \"Prompting_strategy\": \"prompt\",\n",
    "    \"Question_Type\": \"question_type\",\n",
    "    \"Word_Difficulty\": \"word_difficulty\",\n",
    "    \"Task_Difficulty\": \"task_difficulty\",\n",
    "    \"Text\": \"text\",\n",
    "    \"Overall_Score\": \"overall\",\n",
    "    \"Grammar_Score\": \"grammar\",\n",
    "    \"Complexity_Score\": \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\": \"fluency\",\n",
    "    \"sum of scores\": \"sum_scores\",\n",
    "}\n",
    "\n",
    "gemma_df = gemma_df.rename(columns=gemma_rename)\n",
    "\n",
    "# GPT columns:\n",
    "# Row, Prompting_strategy, Question_Type, Question, Correct_Answer,\n",
    "# Word_Difficulty, Task_Difficulty, Text, Overall_Score, Grammar_Score,\n",
    "# Complexity_Score, Readability_Score, Fluency_Score, Error_Count, Errors\n",
    "\n",
    "gpt_rename = {\n",
    "    \"Prompting_strategy\": \"prompt\",\n",
    "    \"Question_Type\": \"question_type\",\n",
    "    \"Word_Difficulty\": \"word_difficulty\",\n",
    "    \"Task_Difficulty\": \"task_difficulty\",\n",
    "    \"Text\": \"text\",\n",
    "    \"Overall_Score\": \"overall\",\n",
    "    \"Grammar_Score\": \"grammar\",\n",
    "    \"Complexity_Score\": \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\": \"fluency\",\n",
    "}\n",
    "\n",
    "gpt_df = gpt_df.rename(columns=gpt_rename)\n",
    "\n",
    "# Add model label\n",
    "gemma_df[\"model\"] = \"Gemma\"\n",
    "gpt_df[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Create a simple item index within each (for reference only; not needed for t-tests)\n",
    "gemma_df[\"item_id\"] = np.arange(len(gemma_df))\n",
    "gpt_df[\"item_id\"]   = np.arange(len(gpt_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc1fed5-ea9a-4f5e-b877-c5b2ddc3c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "     model                               prompt    overall                   \\\n",
      "                                                      mean        std count   \n",
      "0  GPT-3.5                     chain_of_thought  78.582774  13.160470   585   \n",
      "1  GPT-3.5     chain_of_thought_plus_role_chain  78.308277  14.921074   585   \n",
      "2  GPT-3.5     chain_of_thought_plus_sequential  78.538075  15.191198   584   \n",
      "3  GPT-3.5  chain_of_thought_plus_sequential_rl  78.130354  13.463048   585   \n",
      "4  GPT-3.5                             few_shot  81.950241  12.299584   585   \n",
      "5  GPT-3.5                            zero_shot  79.419721  11.157753   585   \n",
      "\n",
      "     grammar                  complexity                  readability  \\\n",
      "        mean        std count       mean        std count        mean   \n",
      "0  94.518048  20.928433   585  92.461538  10.072969   585   78.442540   \n",
      "1  91.927614  24.949208   585  91.025641  12.246373   585   82.233763   \n",
      "2  92.693910  24.695635   584  91.780822  10.953507   584   79.584176   \n",
      "3  93.123339  22.685191   585  93.145299  10.170856   585   75.834069   \n",
      "4  97.605275  13.766789   585  93.401709   9.484179   585   79.677952   \n",
      "5  97.628013  14.255384   585  93.230769  10.236405   585   74.680966   \n",
      "\n",
      "                      fluency                   \n",
      "         std count       mean        std count  \n",
      "0  14.268962   585  32.973697  34.644756   585  \n",
      "1  14.466129   585  34.426753  34.517166   585  \n",
      "2  14.716122   584  35.937555  37.529684   584  \n",
      "3  15.406886   585  35.425724  37.395137   585  \n",
      "4  14.298703   585  41.460994  40.535354   585  \n",
      "5  17.344908   585  33.930845  33.973464   585  \n",
      "\n",
      "Descriptive statistics by model × question type:\n",
      "      model question_type    overall                      grammar             \\\n",
      "                                mean        std count        mean        std   \n",
      "0   GPT-3.5           1.0  71.661125   2.053975   270  100.000000   0.000000   \n",
      "1   GPT-3.5           2.0  70.856790   3.933668   270   99.259259   8.590617   \n",
      "2   GPT-3.5           3.0  74.051516   2.634754   270  100.000000   0.000000   \n",
      "3   GPT-3.5           4.0  95.854859   2.175722   270  100.000000   0.000000   \n",
      "4   GPT-3.5           5.0  95.191433   1.706937   270  100.000000   0.000000   \n",
      "5   GPT-3.5           6.0  77.902103   7.935625   270   97.315286  13.380974   \n",
      "6   GPT-3.5           7.0  87.978158  11.776826   270   97.118876  15.845535   \n",
      "7   GPT-3.5           8.0  77.992927  13.210643   270   92.232360  22.575287   \n",
      "8   GPT-3.5           9.0  74.796734   9.082359   270   98.300958  12.445669   \n",
      "9   GPT-3.5          10.0  67.258592  16.737120   270   78.266117  39.368113   \n",
      "10  GPT-3.5          11.0  84.252370   8.887816   269   98.434047  11.540437   \n",
      "11  GPT-3.5          12.0  68.707232  19.505688   270   71.450617  40.623908   \n",
      "12  GPT-3.5          13.0  82.531116   7.946433   270   97.218837  12.692764   \n",
      "\n",
      "         complexity                  readability                     fluency  \\\n",
      "   count       mean        std count        mean        std count       mean   \n",
      "0    270  80.000000   1.724522   270   77.992648  10.077139   270   0.312976   \n",
      "1    270  80.000000   0.000000   270   75.765434   9.818811   270   0.000000   \n",
      "2    270  79.555556   2.953586   270   87.652778  12.956921   270   3.049247   \n",
      "3    270  99.851852   2.434322   270   93.279297   6.138340   270  86.143148   \n",
      "4    270  99.925926   1.217161   270   94.643186   3.376403   270  81.388051   \n",
      "5    270  94.814815   8.780839   270   80.982949  13.178186   270  19.082177   \n",
      "6    270  91.259259  15.348716   270   81.803339  12.157541   270  72.590440   \n",
      "7    270  95.333333   9.310824   270   65.410194  11.680114   270  44.756390   \n",
      "8    270  94.592593   9.065065   270   63.772537  12.278413   270  19.016624   \n",
      "9    270  91.000000  12.588719   270   78.945983  13.773496   270   9.814745   \n",
      "10   269  98.587361   5.133733   269   79.173585  12.924402   269  46.632810   \n",
      "11   270  97.925926   6.108839   270   79.841173   8.461643   270  22.867826   \n",
      "12   270  99.777778   2.717605   270   60.051219  16.918806   270  58.388906   \n",
      "\n",
      "                     \n",
      "          std count  \n",
      "0    2.675043   270  \n",
      "1    0.000000   270  \n",
      "2   10.109102   270  \n",
      "3    9.735688   270  \n",
      "4    9.445710   270  \n",
      "5   23.466522   270  \n",
      "6   30.614209   270  \n",
      "7   34.341939   270  \n",
      "8   28.970670   270  \n",
      "9   16.783206   270  \n",
      "10  25.289325   269  \n",
      "11  21.079238   270  \n",
      "12  23.569656   270  \n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# 3. Combine + descriptive statistics   #\n",
    "#########################################\n",
    "\n",
    "common_cols = [\n",
    "    \"item_id\",\n",
    "    \"model\",\n",
    "    \"prompt\",\n",
    "    \"question_type\",\n",
    "    \"word_difficulty\",\n",
    "    \"task_difficulty\",\n",
    "    \"overall\",\n",
    "    \"grammar\",\n",
    "    \"complexity\",\n",
    "    \"readability\",\n",
    "    \"fluency\",\n",
    "]\n",
    "\n",
    "# Some cols (e.g., sum_scores) exist only in Gemma; we ignore them for now\n",
    "df = pd.concat(\n",
    "    [\n",
    "        gemma_df[[c for c in common_cols if c in gemma_df.columns]],\n",
    "        gpt_df[[c for c in common_cols if c in gpt_df.columns]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "metric_cols = [\"overall\", \"grammar\", \"complexity\", \"readability\", \"fluency\"]\n",
    "\n",
    "# Descriptive stats by model × prompting strategy\n",
    "desc_model_prompt = (\n",
    "    df.groupby([\"model\", \"prompt\"])[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc_model_prompt)\n",
    "\n",
    "# (Optional) also by model × question type:\n",
    "desc_model_qtype = (\n",
    "    df.groupby([\"model\", \"question_type\"])[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × question type:\")\n",
    "print(desc_model_qtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d5c444-ef40-40b9-9ef6-fba8c025a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "    \"Morph_Complexity\":  \"morph_complexity\",   # new\n",
    "    \"Human_Total\":       \"human_total\",        # new\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1387c-ab3b-41ca-8516-2554ed4d64ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2c675d-f636-4841-8948-4d0bba582c03",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# If your files are Excel:\n",
    "#  - Gemma sheet name: \"Gemma Results\"\n",
    "#  - GPT sheet name:   \"Results\"\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "# Expected columns (based on what you wrote):\n",
    "# Gemma: Prompting_strategy, Question_Type, ..., Overall_Score, Grammar_Score, Complexity_Score,\n",
    "#        Readability_Score, Fluency_Score, Error_Count, Errors, ...\n",
    "# GPT:   Row, Prompting_strategy, Question_Type, ..., Overall_Score, Grammar_Score, Complexity_Score,\n",
    "#        Readability_Score, Fluency_Score, Error_Count, Errors\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Common columns we care about\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset (this will raise a KeyError if any column is missing, so it's a good check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize column names: Prompting_strategy -> prompt\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Also add lowercase metric aliases for convenience\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "gemma = gemma.assign(**{new: gemma[old] for old, new in rename_metrics.items()})\n",
    "gpt   = gpt.assign(**{new: gpt[old]   for old, new in rename_metrics.items()})\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean up types (just to be safe)\n",
    "metric_cols = list(rename_metrics.values())\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "# Descriptives: mean, std, count by model and prompting strategy\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns: e.g. ('overall','mean') -> 'overall_mean'\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            # Not enough data for a reasonable t-test\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        # Cohen's d (pooled SD)\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Optional: per-prompt independent t-tests (robust version)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Welch t-test for each prompt separately, for one metric.\"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Require at least 2 items per group to be meaningful\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run for each metric and concatenate\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5):\")\n",
    "    print(per_prompt_tests.sort_values([\"metric\", \"prompt\"]))\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###########################################\n",
    "# 7. Save outputs (optional, for your paper)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "856f620a-f2c4-49ed-90d3-31b03ca29a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors', 'Unnamed: 17']\n",
      "GPT columns:   ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Combined df head:\n",
      "       prompt  Question_Type  Overall_Score  Grammar_Score  Complexity_Score  \\\n",
      "0  CoT+Seq_Rl            1.0         73.589          100.0              80.0   \n",
      "1  CoT+Seq_Rl            1.0         67.949          100.0              80.0   \n",
      "2  CoT+Seq_Rl            1.0         65.129          100.0              80.0   \n",
      "3  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "4  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "\n",
      "   Readability_Score  Fluency_Score  model  overall  grammar  complexity  \\\n",
      "0             87.945            0.0  Gemma   73.589    100.0        80.0   \n",
      "1             59.745            0.0  Gemma   67.949    100.0        80.0   \n",
      "2             45.645            0.0  Gemma   65.129    100.0        80.0   \n",
      "3             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "4             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "\n",
      "   readability  fluency  \n",
      "0       87.945      0.0  \n",
      "1       59.745      0.0  \n",
      "2       45.645      0.0  \n",
      "3       73.845      0.0  \n",
      "4       73.845      0.0  \n",
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "      model                               prompt  overall_mean  overall_std  \\\n",
      "0   GPT-3.5                     chain_of_thought     78.582774    13.160470   \n",
      "1   GPT-3.5     chain_of_thought_plus_role_chain     78.308277    14.921074   \n",
      "2   GPT-3.5     chain_of_thought_plus_sequential     78.538075    15.191198   \n",
      "3   GPT-3.5  chain_of_thought_plus_sequential_rl     78.130354    13.463048   \n",
      "4   GPT-3.5                             few_shot     81.950241    12.299584   \n",
      "5   GPT-3.5                            zero_shot     79.419721    11.157753   \n",
      "6     Gemma                                  CoT     77.430745    16.626290   \n",
      "7     Gemma                               CoT+RC     77.116496    14.204474   \n",
      "8     Gemma                              CoT+Seq     77.382183    14.261398   \n",
      "9     Gemma                           CoT+Seq_Rl     77.516947    13.606013   \n",
      "10    Gemma                             Few_Shot     76.537508    15.677275   \n",
      "11    Gemma                            Zero_Shot     75.863108    17.447460   \n",
      "\n",
      "    overall_count  grammar_mean  grammar_std  grammar_count  complexity_mean  \\\n",
      "0             585     94.518048    20.928433            585        92.461538   \n",
      "1             585     91.927614    24.949208            585        91.025641   \n",
      "2             584     92.693910    24.695635            584        91.780822   \n",
      "3             585     93.123339    22.685191            585        93.145299   \n",
      "4             585     97.605275    13.766789            585        93.401709   \n",
      "5             585     97.628013    14.255384            585        93.230769   \n",
      "6             391     91.132258    27.511699            391        91.543052   \n",
      "7             329     94.860459    22.056881            329        90.030395   \n",
      "8             392     94.613720    22.227061            392        90.714286   \n",
      "9             394     95.643865    20.030415            394        91.015228   \n",
      "10            328     91.758039    26.666585            328        91.097561   \n",
      "11            309     90.193625    29.488780            309        90.679612   \n",
      "\n",
      "    complexity_std  complexity_count  readability_mean  readability_std  \\\n",
      "0        10.072969               585         78.442540        14.268962   \n",
      "1        12.246373               585         82.233763        14.466129   \n",
      "2        10.953507               584         79.584176        14.716122   \n",
      "3        10.170856               585         75.834069        15.406886   \n",
      "4         9.484179               585         79.677952        14.298703   \n",
      "5        10.236405               585         74.680966        17.344908   \n",
      "6        10.420247               391         77.065462        16.531545   \n",
      "7        10.255828               329         76.245814        16.500288   \n",
      "8        10.289915               392         76.242368        16.333600   \n",
      "9        10.262944               394         75.446773        16.330100   \n",
      "10       10.197571               328         76.011262        17.100958   \n",
      "11       10.864765               309         76.522207        16.929640   \n",
      "\n",
      "    readability_count  fluency_mean  fluency_std  fluency_count  \n",
      "0                 585     32.973697    34.644756            585  \n",
      "1                 585     34.426753    34.517166            585  \n",
      "2                 584     35.937555    37.529684            584  \n",
      "3                 585     35.425724    37.395137            585  \n",
      "4                 585     41.460994    40.535354            585  \n",
      "5                 585     33.930845    33.973464            585  \n",
      "6                 391     36.280694    40.314007            391  \n",
      "7                 329     29.585356    38.180175            329  \n",
      "8                 392     30.726823    37.591094            392  \n",
      "9                 394     29.835004    37.639310            394  \n",
      "10                328     32.062640    38.697610            328  \n",
      "11                309     31.726471    38.439813            309  \n",
      "\n",
      "Global t-tests (Gemma vs GPT-3.5) across ALL prompts:\n",
      "        metric  n_gemma  n_gpt  mean_gemma   mean_gpt    t_stat       p_value  \\\n",
      "0      overall     2144   3509   77.687614  79.155083 -1.898074  5.779893e-02   \n",
      "1      grammar     2144   3509   93.088401  94.583238 -2.329405  1.988795e-02   \n",
      "2   complexity     2144   3509   90.833644  92.507837 -5.796789  7.215443e-09   \n",
      "3  readability     2144   3509   76.291773  78.408576 -4.771811  1.887313e-06   \n",
      "4      fluency     2144   3509   31.829414  35.692525 -3.718400  2.030226e-04   \n",
      "\n",
      "   cohens_d  \n",
      "0 -0.062188  \n",
      "1 -0.066603  \n",
      "2 -0.158575  \n",
      "3 -0.133526  \n",
      "4 -0.103310  \n",
      "Metric overall: 0 per-prompt rows\n",
      "Metric grammar: 0 per-prompt rows\n",
      "Metric complexity: 0 per-prompt rows\n",
      "Metric readability: 0 per-prompt rows\n",
      "Metric fluency: 0 per-prompt rows\n",
      "\n",
      "No per-prompt t-tests could be computed (likely < 2 items per model per prompt for each metric).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# Files and sheet names you specified\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Columns we care about from each file\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset safely (will raise if a column is missing → good sanity check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize prompt column name\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Create lowercase metric aliases (easier to work with)\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "for old, new in rename_metrics.items():\n",
    "    gemma[new] = gemma[old]\n",
    "    gpt[new]   = gpt[old]\n",
    "\n",
    "metric_cols = list(rename_metrics.values())\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean numeric types\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Per-prompt independent t-tests (with safe handling)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Welch t-test for each prompt separately, for one metric.\"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Need at least 2 per group for a meaningful test\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    if not res_m.empty:\n",
    "        per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5):\")\n",
    "    print(\"Columns:\", per_prompt_tests.columns.tolist())\n",
    "\n",
    "    # Only sort by columns that actually exist → avoids KeyError\n",
    "    sort_cols = [c for c in [\"metric\", \"prompt\"] if c in per_prompt_tests.columns]\n",
    "    if sort_cols:\n",
    "        print(per_prompt_tests.sort_values(sort_cols))\n",
    "    else:\n",
    "        print(per_prompt_tests)\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###########################################\n",
    "# 7. Save outputs (optional)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a465fbba-afcd-4e2e-a61f-c9cd78ed23b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors', 'Unnamed: 17']\n",
      "GPT columns:   ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Combined df head:\n",
      "       prompt  Question_Type  Overall_Score  Grammar_Score  Complexity_Score  \\\n",
      "0  CoT+Seq_Rl            1.0         73.589          100.0              80.0   \n",
      "1  CoT+Seq_Rl            1.0         67.949          100.0              80.0   \n",
      "2  CoT+Seq_Rl            1.0         65.129          100.0              80.0   \n",
      "3  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "4  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "\n",
      "   Readability_Score  Fluency_Score  model  overall  grammar  complexity  \\\n",
      "0             87.945            0.0  Gemma   73.589    100.0        80.0   \n",
      "1             59.745            0.0  Gemma   67.949    100.0        80.0   \n",
      "2             45.645            0.0  Gemma   65.129    100.0        80.0   \n",
      "3             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "4             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "\n",
      "   readability  fluency  \n",
      "0       87.945      0.0  \n",
      "1       59.745      0.0  \n",
      "2       45.645      0.0  \n",
      "3       73.845      0.0  \n",
      "4       73.845      0.0  \n",
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "      model                               prompt  overall_mean  overall_std  \\\n",
      "0   GPT-3.5                     chain_of_thought     78.582774    13.160470   \n",
      "1   GPT-3.5     chain_of_thought_plus_role_chain     78.308277    14.921074   \n",
      "2   GPT-3.5     chain_of_thought_plus_sequential     78.538075    15.191198   \n",
      "3   GPT-3.5  chain_of_thought_plus_sequential_rl     78.130354    13.463048   \n",
      "4   GPT-3.5                             few_shot     81.950241    12.299584   \n",
      "5   GPT-3.5                            zero_shot     79.419721    11.157753   \n",
      "6     Gemma                                  CoT     77.430745    16.626290   \n",
      "7     Gemma                               CoT+RC     77.116496    14.204474   \n",
      "8     Gemma                              CoT+Seq     77.382183    14.261398   \n",
      "9     Gemma                           CoT+Seq_Rl     77.516947    13.606013   \n",
      "10    Gemma                             Few_Shot     76.537508    15.677275   \n",
      "11    Gemma                            Zero_Shot     75.863108    17.447460   \n",
      "\n",
      "    overall_count  grammar_mean  grammar_std  grammar_count  complexity_mean  \\\n",
      "0             585     94.518048    20.928433            585        92.461538   \n",
      "1             585     91.927614    24.949208            585        91.025641   \n",
      "2             584     92.693910    24.695635            584        91.780822   \n",
      "3             585     93.123339    22.685191            585        93.145299   \n",
      "4             585     97.605275    13.766789            585        93.401709   \n",
      "5             585     97.628013    14.255384            585        93.230769   \n",
      "6             391     91.132258    27.511699            391        91.543052   \n",
      "7             329     94.860459    22.056881            329        90.030395   \n",
      "8             392     94.613720    22.227061            392        90.714286   \n",
      "9             394     95.643865    20.030415            394        91.015228   \n",
      "10            328     91.758039    26.666585            328        91.097561   \n",
      "11            309     90.193625    29.488780            309        90.679612   \n",
      "\n",
      "    complexity_std  complexity_count  readability_mean  readability_std  \\\n",
      "0        10.072969               585         78.442540        14.268962   \n",
      "1        12.246373               585         82.233763        14.466129   \n",
      "2        10.953507               584         79.584176        14.716122   \n",
      "3        10.170856               585         75.834069        15.406886   \n",
      "4         9.484179               585         79.677952        14.298703   \n",
      "5        10.236405               585         74.680966        17.344908   \n",
      "6        10.420247               391         77.065462        16.531545   \n",
      "7        10.255828               329         76.245814        16.500288   \n",
      "8        10.289915               392         76.242368        16.333600   \n",
      "9        10.262944               394         75.446773        16.330100   \n",
      "10       10.197571               328         76.011262        17.100958   \n",
      "11       10.864765               309         76.522207        16.929640   \n",
      "\n",
      "    readability_count  fluency_mean  fluency_std  fluency_count  \n",
      "0                 585     32.973697    34.644756            585  \n",
      "1                 585     34.426753    34.517166            585  \n",
      "2                 584     35.937555    37.529684            584  \n",
      "3                 585     35.425724    37.395137            585  \n",
      "4                 585     41.460994    40.535354            585  \n",
      "5                 585     33.930845    33.973464            585  \n",
      "6                 391     36.280694    40.314007            391  \n",
      "7                 329     29.585356    38.180175            329  \n",
      "8                 392     30.726823    37.591094            392  \n",
      "9                 394     29.835004    37.639310            394  \n",
      "10                328     32.062640    38.697610            328  \n",
      "11                309     31.726471    38.439813            309  \n",
      "\n",
      "Global t-tests (Gemma vs GPT-3.5) across ALL prompts:\n",
      "        metric  n_gemma  n_gpt  mean_gemma   mean_gpt    t_stat       p_value  \\\n",
      "0      overall     2144   3509   77.687614  79.155083 -1.898074  5.779893e-02   \n",
      "1      grammar     2144   3509   93.088401  94.583238 -2.329405  1.988795e-02   \n",
      "2   complexity     2144   3509   90.833644  92.507837 -5.796789  7.215443e-09   \n",
      "3  readability     2144   3509   76.291773  78.408576 -4.771811  1.887313e-06   \n",
      "4      fluency     2144   3509   31.829414  35.692525 -3.718400  2.030226e-04   \n",
      "\n",
      "   cohens_d  \n",
      "0 -0.062188  \n",
      "1 -0.066603  \n",
      "2 -0.158575  \n",
      "3 -0.133526  \n",
      "4 -0.103310  \n",
      "Metric overall: 0 per-prompt rows\n",
      "Metric grammar: 0 per-prompt rows\n",
      "Metric complexity: 0 per-prompt rows\n",
      "Metric readability: 0 per-prompt rows\n",
      "Metric fluency: 0 per-prompt rows\n",
      "\n",
      "No per-prompt t-tests could be computed (likely < 2 items per model per prompt for each metric).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-1.1503, p-value=0.250423, Cohen's d=-0.079\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=-2.0665, p-value=0.039162, Cohen's d=-0.142\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-1.3675, p-value=0.171858, Cohen's d=-0.090\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-1.3458, p-value=0.178768, Cohen's d=-0.091\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=1.3272, p-value=0.184836, Cohen's d=0.089\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-0.8415, p-value=0.400349, Cohen's d=-0.056\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=-0.4592, p-value=0.646210, Cohen's d=-0.031\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=0.7080, p-value=0.479120, Cohen's d=0.045\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-5.0278, p-value=0.000001, Cohen's d=-0.337\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=0.7450, p-value=0.456525, Cohen's d=0.050\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-1.0548, p-value=0.291856, Cohen's d=-0.070\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=-0.9046, p-value=0.365945, Cohen's d=-0.060\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-0.3421, p-value=0.732380, Cohen's d=-0.022\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-2.4352, p-value=0.015111, Cohen's d=-0.163\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=0.1339, p-value=0.893525, Cohen's d=0.009\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-0.6938, p-value=0.488035, Cohen's d=-0.047\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=-1.1866, p-value=0.235765, Cohen's d=-0.081\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-2.3765, p-value=0.017704, Cohen's d=-0.156\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=1.1716, p-value=0.241718, Cohen's d=0.078\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=0.3341, p-value=0.738362, Cohen's d=0.022\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-4.5993, p-value=0.000005, Cohen's d=-0.318\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-4.3060, p-value=0.000020, Cohen's d=-0.317\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-2.8296, p-value=0.004780, Cohen's d=-0.188\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-2.5514, p-value=0.010925, Cohen's d=-0.172\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-1.9628, p-value=0.049995, Cohen's d=-0.128\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=391, Mean1=77.43\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-2.0739, p-value=0.038502, Cohen's d=-0.146\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=391, Mean1=91.13\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-4.2989, p-value=0.000020, Cohen's d=-0.315\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=391, Mean1=91.54\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-2.4970, p-value=0.012717, Cohen's d=-0.164\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=391, Mean1=77.07\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=2.1648, p-value=0.030673, Cohen's d=0.140\n",
      "\n",
      "### Comparison: Gemma-CoT  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=391, Mean1=36.28\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=0.9491, p-value=0.342866, Cohen's d=0.064\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-1.5376, p-value=0.124634, Cohen's d=-0.108\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=0.2294, p-value=0.818610, Cohen's d=0.016\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-3.4620, p-value=0.000570, Cohen's d=-0.240\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-2.0261, p-value=0.043200, Cohen's d=-0.145\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=-1.3308, p-value=0.183736, Cohen's d=-0.094\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-1.1955, p-value=0.232307, Cohen's d=-0.081\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=1.8392, p-value=0.066277, Cohen's d=0.122\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=-1.3113, p-value=0.190150, Cohen's d=-0.086\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-5.5001, p-value=0.000000, Cohen's d=-0.393\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=-1.9037, p-value=0.057405, Cohen's d=-0.135\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-1.4156, p-value=0.157319, Cohen's d=-0.096\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=1.3640, p-value=0.172988, Cohen's d=0.091\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-2.4155, p-value=0.015963, Cohen's d=-0.163\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-3.0496, p-value=0.002390, Cohen's d=-0.217\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=-2.4284, p-value=0.015429, Cohen's d=-0.168\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-1.0552, p-value=0.291709, Cohen's d=-0.074\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=1.1311, p-value=0.258384, Cohen's d=0.077\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-4.4205, p-value=0.000011, Cohen's d=-0.305\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=0.3708, p-value=0.710938, Cohen's d=0.026\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=-2.2362, p-value=0.025668, Cohen's d=-0.155\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-5.1768, p-value=0.000000, Cohen's d=-0.371\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-2.0443, p-value=0.041473, Cohen's d=-0.159\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-4.8996, p-value=0.000001, Cohen's d=-0.345\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-3.1635, p-value=0.001637, Cohen's d=-0.227\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-4.4137, p-value=0.000012, Cohen's d=-0.299\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=329, Mean1=77.12\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-2.5341, p-value=0.011546, Cohen's d=-0.187\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=329, Mean1=94.86\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-2.0480, p-value=0.041097, Cohen's d=-0.158\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=329, Mean1=90.03\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-4.5314, p-value=0.000007, Cohen's d=-0.312\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=329, Mean1=76.25\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=1.3509, p-value=0.177154, Cohen's d=0.092\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+RC  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=329, Mean1=29.59\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=-1.7172, p-value=0.086445, Cohen's d=-0.122\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-1.3300, p-value=0.183913, Cohen's d=-0.088\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=0.0675, p-value=0.946202, Cohen's d=0.004\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-2.6235, p-value=0.008863, Cohen's d=-0.172\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-2.1694, p-value=0.030365, Cohen's d=-0.145\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=-0.9447, p-value=0.345090, Cohen's d=-0.063\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-0.9765, p-value=0.329090, Cohen's d=-0.063\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=1.7619, p-value=0.078432, Cohen's d=0.112\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=-0.4291, p-value=0.667943, Cohen's d=-0.027\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-5.8798, p-value=0.000000, Cohen's d=-0.393\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=-1.5577, p-value=0.119694, Cohen's d=-0.103\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-1.2090, p-value=0.226972, Cohen's d=-0.078\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=1.2646, p-value=0.206338, Cohen's d=0.081\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-1.5466, p-value=0.122323, Cohen's d=-0.100\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-3.2591, p-value=0.001166, Cohen's d=-0.217\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=-2.1243, p-value=0.033934, Cohen's d=-0.139\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-0.8219, p-value=0.411390, Cohen's d=-0.054\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=1.0188, p-value=0.308586, Cohen's d=0.066\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-3.6363, p-value=0.000294, Cohen's d=-0.238\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=0.3917, p-value=0.695356, Cohen's d=0.026\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=-1.9191, p-value=0.055315, Cohen's d=-0.125\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-5.1808, p-value=0.000000, Cohen's d=-0.348\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-2.3767, p-value=0.017784, Cohen's d=-0.169\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-4.1278, p-value=0.000041, Cohen's d=-0.274\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-3.3851, p-value=0.000748, Cohen's d=-0.227\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-4.2386, p-value=0.000025, Cohen's d=-0.273\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=392, Mean1=77.38\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-2.3821, p-value=0.017483, Cohen's d=-0.163\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=392, Mean1=94.61\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-2.3773, p-value=0.017749, Cohen's d=-0.169\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=392, Mean1=90.71\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-3.7546, p-value=0.000186, Cohen's d=-0.245\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=392, Mean1=76.24\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=1.4284, p-value=0.153526, Cohen's d=0.092\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=392, Mean1=30.73\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=-1.3566, p-value=0.175289, Cohen's d=-0.090\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-1.2179, p-value=0.223629, Cohen's d=-0.080\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=0.8469, p-value=0.397270, Cohen's d=0.055\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-2.1785, p-value=0.029650, Cohen's d=-0.142\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-2.9592, p-value=0.003180, Cohen's d=-0.198\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=-1.3208, p-value=0.186963, Cohen's d=-0.087\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-0.8581, p-value=0.391069, Cohen's d=-0.055\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=2.5753, p-value=0.010166, Cohen's d=0.161\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=-0.0144, p-value=0.988523, Cohen's d=-0.001\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-6.6727, p-value=0.000000, Cohen's d=-0.445\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=-1.9348, p-value=0.053372, Cohen's d=-0.128\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-1.0979, p-value=0.272535, Cohen's d=-0.070\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=2.0540, p-value=0.040249, Cohen's d=0.129\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-1.1135, p-value=0.265819, Cohen's d=-0.072\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-4.0422, p-value=0.000058, Cohen's d=-0.269\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=-2.4898, p-value=0.012974, Cohen's d=-0.162\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-0.6947, p-value=0.487446, Cohen's d=-0.045\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=1.8295, p-value=0.067646, Cohen's d=0.116\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-3.1961, p-value=0.001445, Cohen's d=-0.209\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=-0.3722, p-value=0.709820, Cohen's d=-0.025\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=-2.2850, p-value=0.022559, Cohen's d=-0.149\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-5.1943, p-value=0.000000, Cohen's d=-0.345\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-1.6930, p-value=0.090952, Cohen's d=-0.118\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-3.6777, p-value=0.000251, Cohen's d=-0.243\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-4.1766, p-value=0.000033, Cohen's d=-0.279\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-4.5940, p-value=0.000005, Cohen's d=-0.295\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=394, Mean1=77.52\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-2.3029, p-value=0.021563, Cohen's d=-0.156\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=394, Mean1=95.64\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-1.6978, p-value=0.090012, Cohen's d=-0.118\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=394, Mean1=91.02\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-3.3158, p-value=0.000953, Cohen's d=-0.216\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=394, Mean1=75.45\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=0.7017, p-value=0.483059, Cohen's d=0.045\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-CoT+Seq_Rl  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=394, Mean1=29.84\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=-1.7357, p-value=0.083016, Cohen's d=-0.115\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-2.0004, p-value=0.045921, Cohen's d=-0.145\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=-1.6161, p-value=0.106646, Cohen's d=-0.119\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-1.9476, p-value=0.051884, Cohen's d=-0.135\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-2.1837, p-value=0.029384, Cohen's d=-0.158\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=-0.3542, p-value=0.723336, Cohen's d=-0.025\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-1.6659, p-value=0.096221, Cohen's d=-0.117\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=-0.0943, p-value=0.924881, Cohen's d=-0.007\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=0.0950, p-value=0.924358, Cohen's d=0.006\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-5.5671, p-value=0.000000, Cohen's d=-0.402\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=-0.9201, p-value=0.357894, Cohen's d=-0.066\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-1.8700, p-value=0.061922, Cohen's d=-0.130\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=-0.5222, p-value=0.601738, Cohen's d=-0.037\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-0.9453, p-value=0.344847, Cohen's d=-0.064\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-3.1799, p-value=0.001549, Cohen's d=-0.229\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=-1.4670, p-value=0.142864, Cohen's d=-0.102\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-1.5477, p-value=0.122219, Cohen's d=-0.111\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=-0.7821, p-value=0.434490, Cohen's d=-0.056\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-2.9138, p-value=0.003688, Cohen's d=-0.201\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=0.1556, p-value=0.876425, Cohen's d=0.011\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=-1.2751, p-value=0.202709, Cohen's d=-0.089\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-5.3914, p-value=0.000000, Cohen's d=-0.398\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-3.7041, p-value=0.000240, Cohen's d=-0.301\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-3.3581, p-value=0.000832, Cohen's d=-0.236\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-3.2913, p-value=0.001057, Cohen's d=-0.239\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-3.4609, p-value=0.000571, Cohen's d=-0.236\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=328, Mean1=76.54\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-2.9384, p-value=0.003447, Cohen's d=-0.222\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=328, Mean1=91.76\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-3.7011, p-value=0.000242, Cohen's d=-0.299\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=328, Mean1=91.10\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-3.0285, p-value=0.002551, Cohen's d=-0.209\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=328, Mean1=76.01\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=1.1220, p-value=0.262273, Cohen's d=0.077\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Few_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=328, Mean1=32.06\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=-0.7306, p-value=0.465301, Cohen's d=-0.052\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=585, Mean2=78.58\n",
      "T-stat=-2.4027, p-value=0.016639, Cohen's d=-0.184\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=585, Mean2=94.52\n",
      "T-stat=-2.2910, p-value=0.022400, Cohen's d=-0.178\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=585, Mean2=92.46\n",
      "T-stat=-2.3909, p-value=0.017121, Cohen's d=-0.172\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=585, Mean2=78.44\n",
      "T-stat=-1.7003, p-value=0.089651, Cohen's d=-0.126\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=585, Mean2=32.97\n",
      "T-stat=-0.4771, p-value=0.633466, Cohen's d=-0.035\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=585, Mean2=78.31\n",
      "T-stat=-2.0923, p-value=0.036870, Cohen's d=-0.154\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=585, Mean2=91.93\n",
      "T-stat=-0.8805, p-value=0.378978, Cohen's d=-0.065\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=585, Mean2=91.03\n",
      "T-stat=-0.4331, p-value=0.665088, Cohen's d=-0.029\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=585, Mean2=82.23\n",
      "T-stat=-5.0380, p-value=0.000001, Cohen's d=-0.372\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_role_chain\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=585, Mean2=34.43\n",
      "T-stat=-1.0341, p-value=0.301527, Cohen's d=-0.075\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=584, Mean2=78.54\n",
      "T-stat=-2.2768, p-value=0.023175, Cohen's d=-0.167\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=584, Mean2=92.69\n",
      "T-stat=-1.2729, p-value=0.203616, Cohen's d=-0.095\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=584, Mean2=91.78\n",
      "T-stat=-1.4367, p-value=0.151284, Cohen's d=-0.101\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=584, Mean2=79.58\n",
      "T-stat=-2.6872, p-value=0.007421, Cohen's d=-0.197\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=584, Mean2=35.94\n",
      "T-stat=-1.5701, p-value=0.116915, Cohen's d=-0.111\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=585, Mean2=78.13\n",
      "T-stat=-1.9923, p-value=0.046870, Cohen's d=-0.152\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=585, Mean2=93.12\n",
      "T-stat=-1.5243, p-value=0.128049, Cohen's d=-0.116\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=585, Mean2=93.15\n",
      "T-stat=-3.2983, p-value=0.001031, Cohen's d=-0.237\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=585, Mean2=75.83\n",
      "T-stat=0.5959, p-value=0.551442, Cohen's d=0.043\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-chain_of_thought_plus_sequential_rl\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=585, Mean2=35.43\n",
      "T-stat=-1.3813, p-value=0.167696, Cohen's d=-0.098\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=585, Mean2=81.95\n",
      "T-stat=-5.4581, p-value=0.000000, Cohen's d=-0.426\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=585, Mean2=97.61\n",
      "T-stat=-4.1839, p-value=0.000036, Cohen's d=-0.360\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=585, Mean2=93.40\n",
      "T-stat=-3.7189, p-value=0.000220, Cohen's d=-0.273\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=585, Mean2=79.68\n",
      "T-stat=-2.7925, p-value=0.005414, Cohen's d=-0.207\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-few_shot\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=585, Mean2=41.46\n",
      "T-stat=-3.5332, p-value=0.000439, Cohen's d=-0.244\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: overall\n",
      "N1=309, Mean1=75.86\n",
      "N2=585, Mean2=79.42\n",
      "T-stat=-3.2495, p-value=0.001244, Cohen's d=-0.260\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: grammar\n",
      "N1=309, Mean1=90.19\n",
      "N2=585, Mean2=97.63\n",
      "T-stat=-4.1811, p-value=0.000036, Cohen's d=-0.357\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: complexity\n",
      "N1=309, Mean1=90.68\n",
      "N2=585, Mean2=93.23\n",
      "T-stat=-3.4057, p-value=0.000704, Cohen's d=-0.244\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: readability\n",
      "N1=309, Mean1=76.52\n",
      "N2=585, Mean2=74.68\n",
      "T-stat=1.5334, p-value=0.125671, Cohen's d=0.107\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "### Comparison: Gemma-Zero_Shot  vs  GPT-3.5-zero_shot\n",
      "Metric: fluency\n",
      "N1=309, Mean1=31.73\n",
      "N2=585, Mean2=33.93\n",
      "T-stat=-0.8482, p-value=0.396712, Cohen's d=-0.062\n",
      "➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\n",
      "\n",
      "Cross-prompt pairwise t-tests (Gemma vs GPT-3.5):\n",
      "    model1    prompt1   model2           prompt2       metric   n1   n2  \\\n",
      "0    Gemma        CoT  GPT-3.5  chain_of_thought      overall  391  585   \n",
      "1    Gemma        CoT  GPT-3.5  chain_of_thought      grammar  391  585   \n",
      "2    Gemma        CoT  GPT-3.5  chain_of_thought   complexity  391  585   \n",
      "3    Gemma        CoT  GPT-3.5  chain_of_thought  readability  391  585   \n",
      "4    Gemma        CoT  GPT-3.5  chain_of_thought      fluency  391  585   \n",
      "..     ...        ...      ...               ...          ...  ...  ...   \n",
      "175  Gemma  Zero_Shot  GPT-3.5         zero_shot      overall  309  585   \n",
      "176  Gemma  Zero_Shot  GPT-3.5         zero_shot      grammar  309  585   \n",
      "177  Gemma  Zero_Shot  GPT-3.5         zero_shot   complexity  309  585   \n",
      "178  Gemma  Zero_Shot  GPT-3.5         zero_shot  readability  309  585   \n",
      "179  Gemma  Zero_Shot  GPT-3.5         zero_shot      fluency  309  585   \n",
      "\n",
      "         mean1      mean2    t_stat   p_value  cohens_d  \n",
      "0    77.430745  78.582774 -1.150273  0.250423 -0.078653  \n",
      "1    91.132258  94.518048 -2.066464  0.039162 -0.142355  \n",
      "2    91.543052  92.461538 -1.367459  0.171858 -0.089929  \n",
      "3    77.065462  78.442540 -1.345816  0.178768 -0.090506  \n",
      "4    36.280694  32.973697  1.327234  0.184836  0.089332  \n",
      "..         ...        ...       ...       ...       ...  \n",
      "175  75.863108  79.419721 -3.249477  0.001244 -0.260350  \n",
      "176  90.193625  97.628013 -4.181126  0.000036 -0.357147  \n",
      "177  90.679612  93.230769 -3.405680  0.000704 -0.243952  \n",
      "178  76.522207  74.680966  1.533401  0.125671  0.107032  \n",
      "179  31.726471  33.930845 -0.848155  0.396712 -0.061957  \n",
      "\n",
      "[180 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# Files and sheet names you specified\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Columns we care about from each file\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset safely (will raise if a column is missing → good sanity check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize prompt column name\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Create lowercase metric aliases (easier to work with)\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "for old, new in rename_metrics.items():\n",
    "    gemma[new] = gemma[old]\n",
    "    gpt[new]   = gpt[old]\n",
    "\n",
    "metric_cols = list(rename_metrics.values())\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean numeric types\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Per-prompt independent t-tests (with safe handling)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Welch t-test for each prompt separately, for one metric.\"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Need at least 2 per group for a meaningful test\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    if not res_m.empty:\n",
    "        per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5):\")\n",
    "    print(\"Columns:\", per_prompt_tests.columns.tolist())\n",
    "\n",
    "    # Only sort by columns that actually exist → avoids KeyError\n",
    "    sort_cols = [c for c in [\"metric\", \"prompt\"] if c in per_prompt_tests.columns]\n",
    "    if sort_cols:\n",
    "        print(per_prompt_tests.sort_values(sort_cols))\n",
    "    else:\n",
    "        print(per_prompt_tests)\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###########################################################\n",
    "# 7. Cross-prompt pairwise t-tests (Gemma vs GPT-3.5)\n",
    "###########################################################\n",
    "\n",
    "def compare_prompt_pairs(df, \n",
    "                         model1, prompt1, \n",
    "                         model2, prompt2, \n",
    "                         metric):\n",
    "    \"\"\"\n",
    "    Compare Model1-Prompt1 vs Model2-Prompt2 on a specific metric.\n",
    "    Uses Welch's t-test and computes Cohen's d.\n",
    "    Returns a dict with results (or None if insufficient data).\n",
    "    \"\"\"\n",
    "    mask1 = (df[\"model\"] == model1) & (df[\"prompt\"] == prompt1)\n",
    "    mask2 = (df[\"model\"] == model2) & (df[\"prompt\"] == prompt2)\n",
    "\n",
    "    vals1 = df.loc[mask1, metric].dropna().values\n",
    "    vals2 = df.loc[mask2, metric].dropna().values\n",
    "\n",
    "    if len(vals1) < 2 or len(vals2) < 2:\n",
    "        print(f\"\\n[SKIP] Not enough data for {model1}-{prompt1} vs {model2}-{prompt2} on {metric}\")\n",
    "        return None\n",
    "\n",
    "    t_stat, p_val = stats.ttest_ind(vals1, vals2, equal_var=False)\n",
    "\n",
    "    mean1, mean2 = vals1.mean(), vals2.mean()\n",
    "    sd1, sd2 = vals1.std(ddof=1), vals2.std(ddof=1)\n",
    "    n1, n2 = len(vals1), len(vals2)\n",
    "\n",
    "    pooled_sd = np.sqrt(((n1 - 1)*sd1**2 + (n2 - 1)*sd2**2) / (n1 + n2 - 2))\n",
    "    cohens_d = (mean1 - mean2) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "    print(f\"\\n### Comparison: {model1}-{prompt1}  vs  {model2}-{prompt2}\")\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"N1={n1}, Mean1={mean1:.2f}\")\n",
    "    print(f\"N2={n2}, Mean2={mean2:.2f}\")\n",
    "    print(f\"T-stat={t_stat:.4f}, p-value={p_val:.6f}, Cohen's d={cohens_d:.3f}\")\n",
    "\n",
    "    if p_val >= 0.05:\n",
    "        print(\"➡ The second combo does *NOT* significantly outperform the first (p ≥ 0.05).\")\n",
    "\n",
    "    return {\n",
    "        \"model1\": model1,\n",
    "        \"prompt1\": prompt1,\n",
    "        \"model2\": model2,\n",
    "        \"prompt2\": prompt2,\n",
    "        \"metric\": metric,\n",
    "        \"n1\": n1,\n",
    "        \"n2\": n2,\n",
    "        \"mean1\": mean1,\n",
    "        \"mean2\": mean2,\n",
    "        \"t_stat\": t_stat,\n",
    "        \"p_value\": p_val,\n",
    "        \"cohens_d\": cohens_d,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Define the specific cross-prompt comparisons you care about\n",
    "# (edit this list as needed based on your actual prompt labels)\n",
    "# ------------------------------------------------------------------\n",
    "cross_prompt_specs = [\n",
    "    # Example: Gemma CoT vs GPT-3.5 zero_shot (overall)\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "    \n",
    "    # Add more pairs as you like...\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "\n",
    "  #\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "#\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "#\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "    \n",
    "    # Example: Gemma CoT+Seq vs GPT-3.5 chain_of_thought_plus_sequential\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "    \n",
    "    # Example: Gemma Few_Shot vs GPT-3.5 few_shot\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "    \n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "]\n",
    "\n",
    "metrics_to_compare = [\"overall\", \"grammar\", \"complexity\", \"readability\", \"fluency\"]\n",
    "\n",
    "cross_prompt_rows = []\n",
    "\n",
    "for spec in cross_prompt_specs:\n",
    "    for m in metrics_to_compare:\n",
    "        res = compare_prompt_pairs(\n",
    "            df,\n",
    "            model1=spec[\"model1\"],\n",
    "            prompt1=spec[\"prompt1\"],\n",
    "            model2=spec[\"model2\"],\n",
    "            prompt2=spec[\"prompt2\"],\n",
    "            metric=m\n",
    "        )\n",
    "        if res is not None:\n",
    "            cross_prompt_rows.append(res)\n",
    "\n",
    "if cross_prompt_rows:\n",
    "    cross_prompt_tests = pd.DataFrame(cross_prompt_rows)\n",
    "    print(\"\\nCross-prompt pairwise t-tests (Gemma vs GPT-3.5):\")\n",
    "    print(cross_prompt_tests)\n",
    "else:\n",
    "    cross_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo cross-prompt tests could be computed (check prompt labels / sample sizes).\")\n",
    "\n",
    "###########################################\n",
    "# 8. Save outputs (optional)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "cross_prompt_tests.to_csv(\"cross_prompt_pairwise_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b462ec45-478b-46ec-b110-bbf5d55cab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors', 'Unnamed: 17']\n",
      "GPT columns:   ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Combined df head:\n",
      "       prompt  Question_Type  Overall_Score  Grammar_Score  Complexity_Score  \\\n",
      "0  CoT+Seq_Rl            1.0         73.589          100.0              80.0   \n",
      "1  CoT+Seq_Rl            1.0         67.949          100.0              80.0   \n",
      "2  CoT+Seq_Rl            1.0         65.129          100.0              80.0   \n",
      "3  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "4  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "\n",
      "   Readability_Score  Fluency_Score  model  overall  grammar  complexity  \\\n",
      "0             87.945            0.0  Gemma   73.589    100.0        80.0   \n",
      "1             59.745            0.0  Gemma   67.949    100.0        80.0   \n",
      "2             45.645            0.0  Gemma   65.129    100.0        80.0   \n",
      "3             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "4             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "\n",
      "   readability  fluency  \n",
      "0       87.945      0.0  \n",
      "1       59.745      0.0  \n",
      "2       45.645      0.0  \n",
      "3       73.845      0.0  \n",
      "4       73.845      0.0  \n",
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "      model                               prompt  overall_mean  overall_std  \\\n",
      "0   GPT-3.5                     chain_of_thought     78.582774    13.160470   \n",
      "1   GPT-3.5     chain_of_thought_plus_role_chain     78.308277    14.921074   \n",
      "2   GPT-3.5     chain_of_thought_plus_sequential     78.538075    15.191198   \n",
      "3   GPT-3.5  chain_of_thought_plus_sequential_rl     78.130354    13.463048   \n",
      "4   GPT-3.5                             few_shot     81.950241    12.299584   \n",
      "5   GPT-3.5                            zero_shot     79.419721    11.157753   \n",
      "6     Gemma                                  CoT     77.430745    16.626290   \n",
      "7     Gemma                               CoT+RC     77.116496    14.204474   \n",
      "8     Gemma                              CoT+Seq     77.382183    14.261398   \n",
      "9     Gemma                           CoT+Seq_Rl     77.516947    13.606013   \n",
      "10    Gemma                             Few_Shot     76.537508    15.677275   \n",
      "11    Gemma                            Zero_Shot     75.863108    17.447460   \n",
      "\n",
      "    overall_count  grammar_mean  grammar_std  grammar_count  complexity_mean  \\\n",
      "0             585     94.518048    20.928433            585        92.461538   \n",
      "1             585     91.927614    24.949208            585        91.025641   \n",
      "2             584     92.693910    24.695635            584        91.780822   \n",
      "3             585     93.123339    22.685191            585        93.145299   \n",
      "4             585     97.605275    13.766789            585        93.401709   \n",
      "5             585     97.628013    14.255384            585        93.230769   \n",
      "6             391     91.132258    27.511699            391        91.543052   \n",
      "7             329     94.860459    22.056881            329        90.030395   \n",
      "8             392     94.613720    22.227061            392        90.714286   \n",
      "9             394     95.643865    20.030415            394        91.015228   \n",
      "10            328     91.758039    26.666585            328        91.097561   \n",
      "11            309     90.193625    29.488780            309        90.679612   \n",
      "\n",
      "    complexity_std  complexity_count  readability_mean  readability_std  \\\n",
      "0        10.072969               585         78.442540        14.268962   \n",
      "1        12.246373               585         82.233763        14.466129   \n",
      "2        10.953507               584         79.584176        14.716122   \n",
      "3        10.170856               585         75.834069        15.406886   \n",
      "4         9.484179               585         79.677952        14.298703   \n",
      "5        10.236405               585         74.680966        17.344908   \n",
      "6        10.420247               391         77.065462        16.531545   \n",
      "7        10.255828               329         76.245814        16.500288   \n",
      "8        10.289915               392         76.242368        16.333600   \n",
      "9        10.262944               394         75.446773        16.330100   \n",
      "10       10.197571               328         76.011262        17.100958   \n",
      "11       10.864765               309         76.522207        16.929640   \n",
      "\n",
      "    readability_count  fluency_mean  fluency_std  fluency_count  \n",
      "0                 585     32.973697    34.644756            585  \n",
      "1                 585     34.426753    34.517166            585  \n",
      "2                 584     35.937555    37.529684            584  \n",
      "3                 585     35.425724    37.395137            585  \n",
      "4                 585     41.460994    40.535354            585  \n",
      "5                 585     33.930845    33.973464            585  \n",
      "6                 391     36.280694    40.314007            391  \n",
      "7                 329     29.585356    38.180175            329  \n",
      "8                 392     30.726823    37.591094            392  \n",
      "9                 394     29.835004    37.639310            394  \n",
      "10                328     32.062640    38.697610            328  \n",
      "11                309     31.726471    38.439813            309  \n",
      "\n",
      "Global t-tests (Gemma vs GPT-3.5) across ALL prompts:\n",
      "        metric  n_gemma  n_gpt  mean_gemma   mean_gpt    t_stat       p_value  \\\n",
      "0      overall     2144   3509   77.687614  79.155083 -1.898074  5.779893e-02   \n",
      "1      grammar     2144   3509   93.088401  94.583238 -2.329405  1.988795e-02   \n",
      "2   complexity     2144   3509   90.833644  92.507837 -5.796789  7.215443e-09   \n",
      "3  readability     2144   3509   76.291773  78.408576 -4.771811  1.887313e-06   \n",
      "4      fluency     2144   3509   31.829414  35.692525 -3.718400  2.030226e-04   \n",
      "\n",
      "   cohens_d  \n",
      "0 -0.062188  \n",
      "1 -0.066603  \n",
      "2 -0.158575  \n",
      "3 -0.133526  \n",
      "4 -0.103310  \n",
      "Metric overall: 0 per-prompt rows\n",
      "Metric grammar: 0 per-prompt rows\n",
      "Metric complexity: 0 per-prompt rows\n",
      "Metric readability: 0 per-prompt rows\n",
      "Metric fluency: 0 per-prompt rows\n",
      "\n",
      "No per-prompt t-tests could be computed (likely < 2 items per model per prompt for each metric).\n",
      "\n",
      "Matched prompt-pair t-tests (Gemma vs GPT-3.5):\n",
      "   model1     prompt1   model2                              prompt2  \\\n",
      "2   Gemma         CoT  GPT-3.5                     chain_of_thought   \n",
      "7   Gemma      CoT+RC  GPT-3.5     chain_of_thought_plus_role_chain   \n",
      "12  Gemma     CoT+Seq  GPT-3.5     chain_of_thought_plus_sequential   \n",
      "17  Gemma  CoT+Seq_Rl  GPT-3.5  chain_of_thought_plus_sequential_rl   \n",
      "22  Gemma    Few_Shot  GPT-3.5                             few_shot   \n",
      "27  Gemma   Zero_Shot  GPT-3.5                            zero_shot   \n",
      "4   Gemma         CoT  GPT-3.5                     chain_of_thought   \n",
      "9   Gemma      CoT+RC  GPT-3.5     chain_of_thought_plus_role_chain   \n",
      "14  Gemma     CoT+Seq  GPT-3.5     chain_of_thought_plus_sequential   \n",
      "19  Gemma  CoT+Seq_Rl  GPT-3.5  chain_of_thought_plus_sequential_rl   \n",
      "24  Gemma    Few_Shot  GPT-3.5                             few_shot   \n",
      "29  Gemma   Zero_Shot  GPT-3.5                            zero_shot   \n",
      "1   Gemma         CoT  GPT-3.5                     chain_of_thought   \n",
      "6   Gemma      CoT+RC  GPT-3.5     chain_of_thought_plus_role_chain   \n",
      "11  Gemma     CoT+Seq  GPT-3.5     chain_of_thought_plus_sequential   \n",
      "16  Gemma  CoT+Seq_Rl  GPT-3.5  chain_of_thought_plus_sequential_rl   \n",
      "21  Gemma    Few_Shot  GPT-3.5                             few_shot   \n",
      "26  Gemma   Zero_Shot  GPT-3.5                            zero_shot   \n",
      "0   Gemma         CoT  GPT-3.5                     chain_of_thought   \n",
      "5   Gemma      CoT+RC  GPT-3.5     chain_of_thought_plus_role_chain   \n",
      "10  Gemma     CoT+Seq  GPT-3.5     chain_of_thought_plus_sequential   \n",
      "15  Gemma  CoT+Seq_Rl  GPT-3.5  chain_of_thought_plus_sequential_rl   \n",
      "20  Gemma    Few_Shot  GPT-3.5                             few_shot   \n",
      "25  Gemma   Zero_Shot  GPT-3.5                            zero_shot   \n",
      "3   Gemma         CoT  GPT-3.5                     chain_of_thought   \n",
      "8   Gemma      CoT+RC  GPT-3.5     chain_of_thought_plus_role_chain   \n",
      "13  Gemma     CoT+Seq  GPT-3.5     chain_of_thought_plus_sequential   \n",
      "18  Gemma  CoT+Seq_Rl  GPT-3.5  chain_of_thought_plus_sequential_rl   \n",
      "23  Gemma    Few_Shot  GPT-3.5                             few_shot   \n",
      "28  Gemma   Zero_Shot  GPT-3.5                            zero_shot   \n",
      "\n",
      "         metric   n1   n2      mean1      mean2    t_stat       p_value  \\\n",
      "2    complexity  391  585  91.543052  92.461538 -1.367459  1.718579e-01   \n",
      "7    complexity  329  585  90.030395  91.025641 -1.311275  1.901496e-01   \n",
      "12   complexity  392  584  90.714286  91.780822 -1.546595  1.223233e-01   \n",
      "17   complexity  394  585  91.015228  93.145299 -3.196124  1.445043e-03   \n",
      "22   complexity  328  585  91.097561  93.401709 -3.358074  8.316275e-04   \n",
      "27   complexity  309  585  90.679612  93.230769 -3.405680  7.044311e-04   \n",
      "4       fluency  391  585  36.280694  32.973697  1.327234  1.848360e-01   \n",
      "9       fluency  329  585  29.585356  34.426753 -1.903729  5.740469e-02   \n",
      "14      fluency  392  584  30.726823  35.937555 -2.124336  3.393437e-02   \n",
      "19      fluency  394  585  29.835004  35.425724 -2.285041  2.255905e-02   \n",
      "24      fluency  328  585  32.062640  41.460994 -3.460917  5.708341e-04   \n",
      "29      fluency  309  585  31.726471  33.930845 -0.848155  3.967117e-01   \n",
      "1       grammar  391  585  91.132258  94.518048 -2.066464  3.916170e-02   \n",
      "6       grammar  329  585  94.860459  91.927614  1.839224  6.627669e-02   \n",
      "11      grammar  392  584  94.613720  92.693910  1.264616  2.063382e-01   \n",
      "16      grammar  394  585  95.643865  93.123339  1.829542  6.764591e-02   \n",
      "21      grammar  328  585  91.758039  97.605275 -3.704062  2.400505e-04   \n",
      "26      grammar  309  585  90.193625  97.628013 -4.181126  3.593087e-05   \n",
      "0       overall  391  585  77.430745  78.582774 -1.150273  2.504227e-01   \n",
      "5       overall  329  585  77.116496  78.308277 -1.195461  2.323071e-01   \n",
      "10      overall  392  584  77.382183  78.538075 -1.209047  2.269720e-01   \n",
      "15      overall  394  585  77.516947  78.130354 -0.694683  4.874464e-01   \n",
      "20      overall  328  585  76.537508  81.950241 -5.391431  1.035713e-07   \n",
      "25      overall  309  585  75.863108  79.419721 -3.249477  1.243929e-03   \n",
      "3   readability  391  585  77.065462  78.442540 -1.345816  1.787680e-01   \n",
      "8   readability  329  585  76.245814  82.233763 -5.500107  5.592725e-08   \n",
      "13  readability  392  584  76.242368  79.584176 -3.259089  1.166254e-03   \n",
      "18  readability  394  585  75.446773  75.834069 -0.372229  7.098201e-01   \n",
      "23  readability  328  585  76.011262  79.677952 -3.291344  1.057308e-03   \n",
      "28  readability  309  585  76.522207  74.680966  1.533401  1.256709e-01   \n",
      "\n",
      "    cohens_d  \n",
      "2  -0.089929  \n",
      "7  -0.086020  \n",
      "12 -0.099750  \n",
      "17 -0.208667  \n",
      "22 -0.236414  \n",
      "27 -0.243952  \n",
      "4   0.089332  \n",
      "9  -0.134942  \n",
      "14 -0.138752  \n",
      "19 -0.149112  \n",
      "24 -0.235634  \n",
      "29 -0.061957  \n",
      "1  -0.142355  \n",
      "6   0.122461  \n",
      "11  0.080883  \n",
      "16  0.116387  \n",
      "21 -0.301250  \n",
      "26 -0.357147  \n",
      "0  -0.078653  \n",
      "5  -0.081254  \n",
      "10 -0.077969  \n",
      "15 -0.045368  \n",
      "20 -0.397738  \n",
      "25 -0.260350  \n",
      "3  -0.090506  \n",
      "8  -0.393193  \n",
      "13 -0.217200  \n",
      "18 -0.024536  \n",
      "23 -0.238663  \n",
      "28  0.107032  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# Files and sheet names you specified\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Columns we care about from each file\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset safely (will raise if a column is missing → good sanity check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize prompt column name\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Create lowercase metric aliases (easier to work with)\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "for old, new in rename_metrics.items():\n",
    "    gemma[new] = gemma[old]\n",
    "    gpt[new]   = gpt[old]\n",
    "\n",
    "metric_cols = list(rename_metrics.values())\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean numeric types\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        # Welch t-test\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Per-prompt independent t-tests (same prompt name in both)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"\n",
    "    Welch t-test for each *prompt name* separately, for one metric,\n",
    "    assuming both models share the same prompt label.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Need at least 2 per group for a meaningful test\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    if not res_m.empty:\n",
    "        per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5), same prompt name:\")\n",
    "    sort_cols = [c for c in [\"metric\", \"prompt\"] if c in per_prompt_tests.columns]\n",
    "    if sort_cols:\n",
    "        print(per_prompt_tests.sort_values(sort_cols))\n",
    "    else:\n",
    "        print(per_prompt_tests)\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###################################################################\n",
    "# 7. Matched prompt-pair t-tests (your CoT ↔ chain_of_thought etc.)\n",
    "###################################################################\n",
    "\n",
    "# Mapping: Gemma prompting → GPT prompting\n",
    "prompt_pairs = {\n",
    "    \"CoT\":        \"chain_of_thought\",\n",
    "    \"CoT+RC\":     \"chain_of_thought_plus_role_chain\",\n",
    "    \"CoT+Seq\":    \"chain_of_thought_plus_sequential\",\n",
    "    \"CoT+Seq_Rl\": \"chain_of_thought_plus_sequential_rl\",\n",
    "    \"Few_Shot\":   \"few_shot\",\n",
    "    \"Zero_Shot\":  \"zero_shot\",\n",
    "}\n",
    "\n",
    "def compare_prompt_pairs(df, metrics, prompt_pairs,\n",
    "                         model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"\n",
    "    For each (Gemma_prompt, GPT_prompt) pair and each metric, run a Welch t-test.\n",
    "    This is what you use to say:\n",
    "    'GPT with X prompting does NOT significantly outperform Gemma with Y prompting.'\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for gemma_prompt, gpt_prompt in prompt_pairs.items():\n",
    "        for metric in metrics:\n",
    "            # Extract values\n",
    "            a_vals = df.loc[\n",
    "                (df[\"model\"] == model_a) & (df[\"prompt\"] == gemma_prompt),\n",
    "                metric\n",
    "            ].dropna().values\n",
    "\n",
    "            b_vals = df.loc[\n",
    "                (df[\"model\"] == model_b) & (df[\"prompt\"] == gpt_prompt),\n",
    "                metric\n",
    "            ].dropna().values\n",
    "\n",
    "            if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "                # Not enough data for this comparison\n",
    "                continue\n",
    "\n",
    "            t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "            mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "            sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "            n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "            pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) /\n",
    "                                (n_a + n_b - 2))\n",
    "            cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"model1\":  model_a,\n",
    "                    \"prompt1\": gemma_prompt,\n",
    "                    \"model2\":  model_b,\n",
    "                    \"prompt2\": gpt_prompt,\n",
    "                    \"metric\":  metric,\n",
    "                    \"n1\":      n_a,\n",
    "                    \"n2\":      n_b,\n",
    "                    \"mean1\":   mean_a,\n",
    "                    \"mean2\":   mean_b,\n",
    "                    \"t_stat\":  t_stat,\n",
    "                    \"p_value\": p_val,\n",
    "                    \"cohens_d\": cohens_d,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pairwise_prompt_tests = compare_prompt_pairs(\n",
    "    df,\n",
    "    metrics=metric_cols,\n",
    "    prompt_pairs=prompt_pairs,\n",
    "    model_a=\"Gemma\",\n",
    "    model_b=\"GPT-3.5\",\n",
    ")\n",
    "\n",
    "print(\"\\nMatched prompt-pair t-tests (Gemma vs GPT-3.5):\")\n",
    "if not pairwise_prompt_tests.empty:\n",
    "    print(pairwise_prompt_tests.sort_values([\"metric\", \"prompt1\", \"prompt2\"]))\n",
    "else:\n",
    "    print(\"No matched prompt-pair tests could be computed (check prompt labels).\")\n",
    "\n",
    "###########################################\n",
    "# 8. Save outputs (optional)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt_same_prompt.csv\", index=False)\n",
    "pairwise_prompt_tests.to_csv(\"pairwise_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79d32bd9-f3dd-4e92-8142-3aa7fc894d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors', 'Unnamed: 17']\n",
      "GPT columns:   ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Combined df head:\n",
      "       prompt  Question_Type  Overall_Score  Grammar_Score  Complexity_Score  \\\n",
      "0  CoT+Seq_Rl            1.0         73.589          100.0              80.0   \n",
      "1  CoT+Seq_Rl            1.0         67.949          100.0              80.0   \n",
      "2  CoT+Seq_Rl            1.0         65.129          100.0              80.0   \n",
      "3  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "4  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "\n",
      "   Readability_Score  Fluency_Score  model  overall  grammar  complexity  \\\n",
      "0             87.945            0.0  Gemma   73.589    100.0        80.0   \n",
      "1             59.745            0.0  Gemma   67.949    100.0        80.0   \n",
      "2             45.645            0.0  Gemma   65.129    100.0        80.0   \n",
      "3             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "4             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "\n",
      "   readability  fluency  \n",
      "0       87.945      0.0  \n",
      "1       59.745      0.0  \n",
      "2       45.645      0.0  \n",
      "3       73.845      0.0  \n",
      "4       73.845      0.0  \n",
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "      model                               prompt  overall_mean  overall_std  \\\n",
      "0   GPT-3.5                     chain_of_thought     78.582774    13.160470   \n",
      "1   GPT-3.5     chain_of_thought_plus_role_chain     78.308277    14.921074   \n",
      "2   GPT-3.5     chain_of_thought_plus_sequential     78.538075    15.191198   \n",
      "3   GPT-3.5  chain_of_thought_plus_sequential_rl     78.130354    13.463048   \n",
      "4   GPT-3.5                             few_shot     81.950241    12.299584   \n",
      "5   GPT-3.5                            zero_shot     79.419721    11.157753   \n",
      "6     Gemma                                  CoT     77.430745    16.626290   \n",
      "7     Gemma                               CoT+RC     77.116496    14.204474   \n",
      "8     Gemma                              CoT+Seq     77.382183    14.261398   \n",
      "9     Gemma                           CoT+Seq_Rl     77.516947    13.606013   \n",
      "10    Gemma                             Few_Shot     76.537508    15.677275   \n",
      "11    Gemma                            Zero_Shot     75.863108    17.447460   \n",
      "\n",
      "    overall_count  grammar_mean  grammar_std  grammar_count  complexity_mean  \\\n",
      "0             585     94.518048    20.928433            585        92.461538   \n",
      "1             585     91.927614    24.949208            585        91.025641   \n",
      "2             584     92.693910    24.695635            584        91.780822   \n",
      "3             585     93.123339    22.685191            585        93.145299   \n",
      "4             585     97.605275    13.766789            585        93.401709   \n",
      "5             585     97.628013    14.255384            585        93.230769   \n",
      "6             391     91.132258    27.511699            391        91.543052   \n",
      "7             329     94.860459    22.056881            329        90.030395   \n",
      "8             392     94.613720    22.227061            392        90.714286   \n",
      "9             394     95.643865    20.030415            394        91.015228   \n",
      "10            328     91.758039    26.666585            328        91.097561   \n",
      "11            309     90.193625    29.488780            309        90.679612   \n",
      "\n",
      "    complexity_std  complexity_count  readability_mean  readability_std  \\\n",
      "0        10.072969               585         78.442540        14.268962   \n",
      "1        12.246373               585         82.233763        14.466129   \n",
      "2        10.953507               584         79.584176        14.716122   \n",
      "3        10.170856               585         75.834069        15.406886   \n",
      "4         9.484179               585         79.677952        14.298703   \n",
      "5        10.236405               585         74.680966        17.344908   \n",
      "6        10.420247               391         77.065462        16.531545   \n",
      "7        10.255828               329         76.245814        16.500288   \n",
      "8        10.289915               392         76.242368        16.333600   \n",
      "9        10.262944               394         75.446773        16.330100   \n",
      "10       10.197571               328         76.011262        17.100958   \n",
      "11       10.864765               309         76.522207        16.929640   \n",
      "\n",
      "    readability_count  fluency_mean  fluency_std  fluency_count  \n",
      "0                 585     32.973697    34.644756            585  \n",
      "1                 585     34.426753    34.517166            585  \n",
      "2                 584     35.937555    37.529684            584  \n",
      "3                 585     35.425724    37.395137            585  \n",
      "4                 585     41.460994    40.535354            585  \n",
      "5                 585     33.930845    33.973464            585  \n",
      "6                 391     36.280694    40.314007            391  \n",
      "7                 329     29.585356    38.180175            329  \n",
      "8                 392     30.726823    37.591094            392  \n",
      "9                 394     29.835004    37.639310            394  \n",
      "10                328     32.062640    38.697610            328  \n",
      "11                309     31.726471    38.439813            309  \n",
      "\n",
      "Global t-tests (Gemma vs GPT-3.5) across ALL prompts:\n",
      "        metric  n_gemma  n_gpt  mean_gemma   mean_gpt    t_stat       p_value  \\\n",
      "0      overall     2144   3509   77.687614  79.155083 -1.898074  5.779893e-02   \n",
      "1      grammar     2144   3509   93.088401  94.583238 -2.329405  1.988795e-02   \n",
      "2   complexity     2144   3509   90.833644  92.507837 -5.796789  7.215443e-09   \n",
      "3  readability     2144   3509   76.291773  78.408576 -4.771811  1.887313e-06   \n",
      "4      fluency     2144   3509   31.829414  35.692525 -3.718400  2.030226e-04   \n",
      "\n",
      "   cohens_d  \n",
      "0 -0.062188  \n",
      "1 -0.066603  \n",
      "2 -0.158575  \n",
      "3 -0.133526  \n",
      "4 -0.103310  \n",
      "Metric overall: 0 per-prompt rows\n",
      "Metric grammar: 0 per-prompt rows\n",
      "Metric complexity: 0 per-prompt rows\n",
      "Metric readability: 0 per-prompt rows\n",
      "Metric fluency: 0 per-prompt rows\n",
      "\n",
      "No per-prompt t-tests could be computed (likely < 2 items per model per prompt for each metric).\n",
      "\n",
      "Cross-prompt t-tests (Gemma vs GPT-3.5) for all specified pairs:\n",
      "    model1    prompt1   model2           prompt2       metric   n1   n2  \\\n",
      "2    Gemma        CoT  GPT-3.5  chain_of_thought   complexity  391  585   \n",
      "4    Gemma        CoT  GPT-3.5  chain_of_thought      fluency  391  585   \n",
      "1    Gemma        CoT  GPT-3.5  chain_of_thought      grammar  391  585   \n",
      "0    Gemma        CoT  GPT-3.5  chain_of_thought      overall  391  585   \n",
      "3    Gemma        CoT  GPT-3.5  chain_of_thought  readability  391  585   \n",
      "..     ...        ...      ...               ...          ...  ...  ...   \n",
      "177  Gemma  Zero_Shot  GPT-3.5         zero_shot   complexity  309  585   \n",
      "179  Gemma  Zero_Shot  GPT-3.5         zero_shot      fluency  309  585   \n",
      "176  Gemma  Zero_Shot  GPT-3.5         zero_shot      grammar  309  585   \n",
      "175  Gemma  Zero_Shot  GPT-3.5         zero_shot      overall  309  585   \n",
      "178  Gemma  Zero_Shot  GPT-3.5         zero_shot  readability  309  585   \n",
      "\n",
      "         mean1      mean2    t_stat   p_value  cohens_d  \n",
      "2    91.543052  92.461538 -1.367459  0.171858 -0.089929  \n",
      "4    36.280694  32.973697  1.327234  0.184836  0.089332  \n",
      "1    91.132258  94.518048 -2.066464  0.039162 -0.142355  \n",
      "0    77.430745  78.582774 -1.150273  0.250423 -0.078653  \n",
      "3    77.065462  78.442540 -1.345816  0.178768 -0.090506  \n",
      "..         ...        ...       ...       ...       ...  \n",
      "177  90.679612  93.230769 -3.405680  0.000704 -0.243952  \n",
      "179  31.726471  33.930845 -0.848155  0.396712 -0.061957  \n",
      "176  90.193625  97.628013 -4.181126  0.000036 -0.357147  \n",
      "175  75.863108  79.419721 -3.249477  0.001244 -0.260350  \n",
      "178  76.522207  74.680966  1.533401  0.125671  0.107032  \n",
      "\n",
      "[180 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# Files and sheet names you specified\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Columns we care about from each file\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset safely (will raise if a column is missing → good sanity check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize prompt column name\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Create lowercase metric aliases (easier to work with)\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "for old, new in rename_metrics.items():\n",
    "    gemma[new] = gemma[old]\n",
    "    gpt[new]   = gpt[old]\n",
    "\n",
    "metric_cols = list(rename_metrics.values())\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean numeric types\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        # Welch t-test\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Per-prompt independent t-tests (same prompt name in both)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"\n",
    "    Welch t-test for each *prompt name* separately, for one metric,\n",
    "    assuming both models share the same prompt label.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Need at least 2 per group for a meaningful test\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"metric\": metric,\n",
    "                \"n_gemma\": n_a,\n",
    "                \"n_gpt\": n_b,\n",
    "                \"mean_gemma\": mean_a,\n",
    "                \"mean_gpt\": mean_b,\n",
    "                \"t_stat\": t_stat,\n",
    "                \"p_value\": p_val,\n",
    "                \"cohens_d\": cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    if not res_m.empty:\n",
    "        per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5), same prompt name:\")\n",
    "    sort_cols = [c for c in [\"metric\", \"prompt\"] if c in per_prompt_tests.columns]\n",
    "    if sort_cols:\n",
    "        print(per_prompt_tests.sort_values(sort_cols))\n",
    "    else:\n",
    "        print(per_prompt_tests)\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###################################################################\n",
    "# 7. Cross-prompt specs: ALL pairs you want to check\n",
    "###################################################################\n",
    "\n",
    "cross_prompt_specs = [\n",
    "    # Gemma CoT vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+RC vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+Seq vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+Seq_Rl vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma Few_Shot vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma Zero_Shot vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "]\n",
    "\n",
    "###################################################################\n",
    "# 8. Run cross-prompt t-tests for ALL metrics over these pairs\n",
    "###################################################################\n",
    "\n",
    "def run_cross_prompt_tests(df, metrics, specs):\n",
    "    \"\"\"\n",
    "    For each spec dict (model1, prompt1, model2, prompt2) and each metric,\n",
    "    run a Welch t-test and compute Cohen's d.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for spec in specs:\n",
    "        m1 = spec[\"model1\"]\n",
    "        p1 = spec[\"prompt1\"]\n",
    "        m2 = spec[\"model2\"]\n",
    "        p2 = spec[\"prompt2\"]\n",
    "\n",
    "        for metric in metrics:\n",
    "            v1 = df.loc[(df[\"model\"] == m1) & (df[\"prompt\"] == p1), metric].dropna().values\n",
    "            v2 = df.loc[(df[\"model\"] == m2) & (df[\"prompt\"] == p2), metric].dropna().values\n",
    "\n",
    "            if len(v1) < 2 or len(v2) < 2:\n",
    "                # Not enough data for a meaningful test\n",
    "                continue\n",
    "\n",
    "            t_stat, p_val = stats.ttest_ind(v1, v2, equal_var=False)\n",
    "\n",
    "            mean1, mean2 = v1.mean(), v2.mean()\n",
    "            sd1, sd2 = v1.std(ddof=1), v2.std(ddof=1)\n",
    "            n1, n2 = len(v1), len(v2)\n",
    "\n",
    "            pooled_sd = np.sqrt(((n1 - 1)*sd1**2 + (n2 - 1)*sd2**2) / (n1 + n2 - 2))\n",
    "            cohens_d = (mean1 - mean2) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"model1\":  m1,\n",
    "                    \"prompt1\": p1,\n",
    "                    \"model2\":  m2,\n",
    "                    \"prompt2\": p2,\n",
    "                    \"metric\":  metric,\n",
    "                    \"n1\":      n1,\n",
    "                    \"n2\":      n2,\n",
    "                    \"mean1\":   mean1,\n",
    "                    \"mean2\":   mean2,\n",
    "                    \"t_stat\":  t_stat,\n",
    "                    \"p_value\": p_val,\n",
    "                    \"cohens_d\": cohens_d,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cross_prompt_tests = run_cross_prompt_tests(df, metrics=metric_cols, specs=cross_prompt_specs)\n",
    "\n",
    "print(\"\\nCross-prompt t-tests (Gemma vs GPT-3.5) for all specified pairs:\")\n",
    "if not cross_prompt_tests.empty:\n",
    "    print(cross_prompt_tests.sort_values([\"prompt1\", \"prompt2\", \"metric\"]))\n",
    "else:\n",
    "    print(\"No cross-prompt tests could be computed (check labels / sample sizes).\")\n",
    "\n",
    "###########################################\n",
    "# 9. Save outputs (optional)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"desc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"global_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"per_prompt_t_tests_gemma_vs_gpt_same_prompt.csv\", index=False)\n",
    "cross_prompt_tests.to_csv(\"cross_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f49761be-7e0e-4ee8-b629-948f34511349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in cross-prompt file: ['model1', 'prompt1', 'model2', 'prompt2', 'metric', 'n1', 'n2', 'mean1', 'mean2', 't_stat', 'p_value', 'cohens_d']\n",
      "\n",
      "LaTeX tabular code:\n",
      "\n",
      "\\begin{tabular}{llllrrllll}\n",
      "\\toprule\n",
      "Model 1 & Prompt 1 & Model 2 & Prompt 2 & $n_1$ & $n_2$ & Mean 1 & Mean 2 & $p$-value & Cohen's $d$ \\\\\n",
      "\\midrule\n",
      "Gemma & CoT & GPT-3.5 & chain_of_thought & 391 & 585 & 77.43 & 78.58 & 0.250 & -0.079 \\\\\n",
      "Gemma & CoT & GPT-3.5 & chain_of_thought_plus_role_chain & 391 & 585 & 77.43 & 78.31 & 0.400 & -0.056 \\\\\n",
      "Gemma & CoT & GPT-3.5 & chain_of_thought_plus_sequential & 391 & 584 & 77.43 & 78.54 & 0.292 & -0.070 \\\\\n",
      "Gemma & CoT & GPT-3.5 & chain_of_thought_plus_sequential_rl & 391 & 585 & 77.43 & 78.13 & 0.488 & -0.047 \\\\\n",
      "Gemma & CoT & GPT-3.5 & few_shot & 391 & 585 & 77.43 & 81.95 & \\textbf{0.000} & -0.318 \\\\\n",
      "Gemma & CoT & GPT-3.5 & zero_shot & 391 & 585 & 77.43 & 79.42 & \\textbf{0.039} & -0.146 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & chain_of_thought & 329 & 585 & 77.12 & 78.58 & 0.125 & -0.108 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & chain_of_thought_plus_role_chain & 329 & 585 & 77.12 & 78.31 & 0.232 & -0.081 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & chain_of_thought_plus_sequential & 329 & 584 & 77.12 & 78.54 & 0.157 & -0.096 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & chain_of_thought_plus_sequential_rl & 329 & 585 & 77.12 & 78.13 & 0.292 & -0.074 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & few_shot & 329 & 585 & 77.12 & 81.95 & \\textbf{0.000} & -0.371 \\\\\n",
      "Gemma & CoT+RC & GPT-3.5 & zero_shot & 329 & 585 & 77.12 & 79.42 & \\textbf{0.012} & -0.187 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & chain_of_thought & 392 & 585 & 77.38 & 78.58 & 0.184 & -0.088 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & chain_of_thought_plus_role_chain & 392 & 585 & 77.38 & 78.31 & 0.329 & -0.063 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & chain_of_thought_plus_sequential & 392 & 584 & 77.38 & 78.54 & 0.227 & -0.078 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & chain_of_thought_plus_sequential_rl & 392 & 585 & 77.38 & 78.13 & 0.411 & -0.054 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & few_shot & 392 & 585 & 77.38 & 81.95 & \\textbf{0.000} & -0.348 \\\\\n",
      "Gemma & CoT+Seq & GPT-3.5 & zero_shot & 392 & 585 & 77.38 & 79.42 & \\textbf{0.017} & -0.163 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & chain_of_thought & 394 & 585 & 77.52 & 78.58 & 0.224 & -0.080 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & chain_of_thought_plus_role_chain & 394 & 585 & 77.52 & 78.31 & 0.391 & -0.055 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & chain_of_thought_plus_sequential & 394 & 584 & 77.52 & 78.54 & 0.273 & -0.070 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & chain_of_thought_plus_sequential_rl & 394 & 585 & 77.52 & 78.13 & 0.487 & -0.045 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & few_shot & 394 & 585 & 77.52 & 81.95 & \\textbf{0.000} & -0.345 \\\\\n",
      "Gemma & CoT+Seq_Rl & GPT-3.5 & zero_shot & 394 & 585 & 77.52 & 79.42 & \\textbf{0.022} & -0.156 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & chain_of_thought & 328 & 585 & 76.54 & 78.58 & \\textbf{0.046} & -0.145 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & chain_of_thought_plus_role_chain & 328 & 585 & 76.54 & 78.31 & 0.096 & -0.117 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & chain_of_thought_plus_sequential & 328 & 584 & 76.54 & 78.54 & 0.062 & -0.130 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & chain_of_thought_plus_sequential_rl & 328 & 585 & 76.54 & 78.13 & 0.122 & -0.111 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & few_shot & 328 & 585 & 76.54 & 81.95 & \\textbf{0.000} & -0.398 \\\\\n",
      "Gemma & Few_Shot & GPT-3.5 & zero_shot & 328 & 585 & 76.54 & 79.42 & \\textbf{0.003} & -0.222 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & chain_of_thought & 309 & 585 & 75.86 & 78.58 & \\textbf{0.017} & -0.184 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & chain_of_thought_plus_role_chain & 309 & 585 & 75.86 & 78.31 & \\textbf{0.037} & -0.154 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & chain_of_thought_plus_sequential & 309 & 584 & 75.86 & 78.54 & \\textbf{0.023} & -0.167 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & chain_of_thought_plus_sequential_rl & 309 & 585 & 75.86 & 78.13 & \\textbf{0.047} & -0.152 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & few_shot & 309 & 585 & 75.86 & 81.95 & \\textbf{0.000} & -0.426 \\\\\n",
      "Gemma & Zero_Shot & GPT-3.5 & zero_shot & 309 & 585 & 75.86 & 79.42 & \\textbf{0.001} & -0.260 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Load cross-prompt t-test results from CSV\n",
    "# ----------------------------------------------------\n",
    "# Make sure the file name matches what you saved previously\n",
    "cross = pd.read_csv(\"cross_prompt_t_tests_gemma_vs_gpt.csv\")\n",
    "\n",
    "print(\"Columns in cross-prompt file:\", cross.columns.tolist())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Filter to 'overall' metric only (if desired)\n",
    "# ----------------------------------------------------\n",
    "overall = cross[cross[\"metric\"] == \"overall\"].copy()\n",
    "\n",
    "# Optional: sort rows for a nice layout\n",
    "overall = overall.sort_values(\n",
    "    by=[\"model1\", \"prompt1\", \"model2\", \"prompt2\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Select columns for the paper table\n",
    "# ----------------------------------------------------\n",
    "cols_for_table = [\n",
    "    \"model1\", \"prompt1\",\n",
    "    \"model2\", \"prompt2\",\n",
    "    \"n1\", \"n2\",\n",
    "    \"mean1\", \"mean2\",\n",
    "    \"p_value\",\n",
    "    \"cohens_d\",\n",
    "]\n",
    "\n",
    "table_df = overall[cols_for_table].copy()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Nicely format numeric columns\n",
    "# ----------------------------------------------------\n",
    "# Adjust decimal places as you prefer\n",
    "table_df[\"mean1\"]   = table_df[\"mean1\"].map(lambda x: f\"{x:.2f}\")\n",
    "table_df[\"mean2\"]   = table_df[\"mean2\"].map(lambda x: f\"{x:.2f}\")\n",
    "table_df[\"p_value\"] = table_df[\"p_value\"].map(lambda x: f\"{x:.3f}\")\n",
    "table_df[\"cohens_d\"] = table_df[\"cohens_d\"].map(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "# Optional: highlight significant p-values in bold (p < 0.05)\n",
    "def format_p(p_str):\n",
    "    try:\n",
    "        val = float(p_str)\n",
    "        if val < 0.05:\n",
    "            return f\"\\\\textbf{{{p_str}}}\"\n",
    "        else:\n",
    "            return p_str\n",
    "    except ValueError:\n",
    "        return p_str\n",
    "\n",
    "table_df[\"p_value\"] = table_df[\"p_value\"].apply(format_p)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. Rename columns for LaTeX header\n",
    "# ----------------------------------------------------\n",
    "table_df = table_df.rename(columns={\n",
    "    \"model1\":   \"Model 1\",\n",
    "    \"prompt1\":  \"Prompt 1\",\n",
    "    \"model2\":   \"Model 2\",\n",
    "    \"prompt2\":  \"Prompt 2\",\n",
    "    \"n1\":       \"$n_1$\",\n",
    "    \"n2\":       \"$n_2$\",\n",
    "    \"mean1\":    \"Mean 1\",\n",
    "    \"mean2\":    \"Mean 2\",\n",
    "    \"p_value\":  \"$p$-value\",\n",
    "    \"cohens_d\": \"Cohen's $d$\",\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6. Export as LaTeX tabular (without table environment)\n",
    "# ----------------------------------------------------\n",
    "latex_tabular = table_df.to_latex(\n",
    "    index=False,\n",
    "    escape=False,     # allow LaTeX commands like \\textbf in cells\n",
    "    column_format=\"llllrrllll\"  # adjust alignment if desired\n",
    ")\n",
    "\n",
    "print(\"\\nLaTeX tabular code:\\n\")\n",
    "print(latex_tabular)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7. (Optional) Write to a .tex file\n",
    "# ----------------------------------------------------\n",
    "with open(\"cross_prompt_overall_ttests_table.tex\", \"w\") as f:\n",
    "    f.write(latex_tabular)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b61057c-d674-447a-9e7c-d733a3f87c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors', 'Unnamed: 17']\n",
      "GPT columns:   ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Combined df head:\n",
      "       prompt  Question_Type  Overall_Score  Grammar_Score  Complexity_Score  \\\n",
      "0  CoT+Seq_Rl            1.0         73.589          100.0              80.0   \n",
      "1  CoT+Seq_Rl            1.0         67.949          100.0              80.0   \n",
      "2  CoT+Seq_Rl            1.0         65.129          100.0              80.0   \n",
      "3  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "4  CoT+Seq_Rl            1.0         70.769          100.0              80.0   \n",
      "\n",
      "   Readability_Score  Fluency_Score  model  overall  grammar  complexity  \\\n",
      "0             87.945            0.0  Gemma   73.589    100.0        80.0   \n",
      "1             59.745            0.0  Gemma   67.949    100.0        80.0   \n",
      "2             45.645            0.0  Gemma   65.129    100.0        80.0   \n",
      "3             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "4             73.845            0.0  Gemma   70.769    100.0        80.0   \n",
      "\n",
      "   readability  fluency  \n",
      "0       87.945      0.0  \n",
      "1       59.745      0.0  \n",
      "2       45.645      0.0  \n",
      "3       73.845      0.0  \n",
      "4       73.845      0.0  \n",
      "\n",
      "Descriptive statistics by model × prompting strategy:\n",
      "      model                               prompt  overall_mean  overall_std  \\\n",
      "0   GPT-3.5                     chain_of_thought     78.582774    13.160470   \n",
      "1   GPT-3.5     chain_of_thought_plus_role_chain     78.308277    14.921074   \n",
      "2   GPT-3.5     chain_of_thought_plus_sequential     78.538075    15.191198   \n",
      "3   GPT-3.5  chain_of_thought_plus_sequential_rl     78.130354    13.463048   \n",
      "4   GPT-3.5                             few_shot     81.950241    12.299584   \n",
      "5   GPT-3.5                            zero_shot     79.419721    11.157753   \n",
      "6     Gemma                                  CoT     77.430745    16.626290   \n",
      "7     Gemma                               CoT+RC     77.116496    14.204474   \n",
      "8     Gemma                              CoT+Seq     77.382183    14.261398   \n",
      "9     Gemma                           CoT+Seq_Rl     77.516947    13.606013   \n",
      "10    Gemma                             Few_Shot     76.537508    15.677275   \n",
      "11    Gemma                            Zero_Shot     75.863108    17.447460   \n",
      "\n",
      "    overall_count  grammar_mean  grammar_std  grammar_count  complexity_mean  \\\n",
      "0             585     94.518048    20.928433            585        92.461538   \n",
      "1             585     91.927614    24.949208            585        91.025641   \n",
      "2             584     92.693910    24.695635            584        91.780822   \n",
      "3             585     93.123339    22.685191            585        93.145299   \n",
      "4             585     97.605275    13.766789            585        93.401709   \n",
      "5             585     97.628013    14.255384            585        93.230769   \n",
      "6             391     91.132258    27.511699            391        91.543052   \n",
      "7             329     94.860459    22.056881            329        90.030395   \n",
      "8             392     94.613720    22.227061            392        90.714286   \n",
      "9             394     95.643865    20.030415            394        91.015228   \n",
      "10            328     91.758039    26.666585            328        91.097561   \n",
      "11            309     90.193625    29.488780            309        90.679612   \n",
      "\n",
      "    complexity_std  complexity_count  readability_mean  readability_std  \\\n",
      "0        10.072969               585         78.442540        14.268962   \n",
      "1        12.246373               585         82.233763        14.466129   \n",
      "2        10.953507               584         79.584176        14.716122   \n",
      "3        10.170856               585         75.834069        15.406886   \n",
      "4         9.484179               585         79.677952        14.298703   \n",
      "5        10.236405               585         74.680966        17.344908   \n",
      "6        10.420247               391         77.065462        16.531545   \n",
      "7        10.255828               329         76.245814        16.500288   \n",
      "8        10.289915               392         76.242368        16.333600   \n",
      "9        10.262944               394         75.446773        16.330100   \n",
      "10       10.197571               328         76.011262        17.100958   \n",
      "11       10.864765               309         76.522207        16.929640   \n",
      "\n",
      "    readability_count  fluency_mean  fluency_std  fluency_count  \n",
      "0                 585     32.973697    34.644756            585  \n",
      "1                 585     34.426753    34.517166            585  \n",
      "2                 584     35.937555    37.529684            584  \n",
      "3                 585     35.425724    37.395137            585  \n",
      "4                 585     41.460994    40.535354            585  \n",
      "5                 585     33.930845    33.973464            585  \n",
      "6                 391     36.280694    40.314007            391  \n",
      "7                 329     29.585356    38.180175            329  \n",
      "8                 392     30.726823    37.591094            392  \n",
      "9                 394     29.835004    37.639310            394  \n",
      "10                328     32.062640    38.697610            328  \n",
      "11                309     31.726471    38.439813            309  \n",
      "\n",
      "Global t-tests (Gemma vs GPT-3.5) across ALL prompts:\n",
      "        metric  n_gemma  n_gpt  mean_gemma   mean_gpt    t_stat       p_value  \\\n",
      "0      overall     2144   3509   77.687614  79.155083 -1.898074  5.779893e-02   \n",
      "1      grammar     2144   3509   93.088401  94.583238 -2.329405  1.988795e-02   \n",
      "2   complexity     2144   3509   90.833644  92.507837 -5.796789  7.215443e-09   \n",
      "3  readability     2144   3509   76.291773  78.408576 -4.771811  1.887313e-06   \n",
      "4      fluency     2144   3509   31.829414  35.692525 -3.718400  2.030226e-04   \n",
      "\n",
      "   cohens_d  \n",
      "0 -0.062188  \n",
      "1 -0.066603  \n",
      "2 -0.158575  \n",
      "3 -0.133526  \n",
      "4 -0.103310  \n",
      "Metric overall: 0 per-prompt rows\n",
      "Metric grammar: 0 per-prompt rows\n",
      "Metric complexity: 0 per-prompt rows\n",
      "Metric readability: 0 per-prompt rows\n",
      "Metric fluency: 0 per-prompt rows\n",
      "\n",
      "No per-prompt t-tests could be computed (likely < 2 items per model per prompt for each metric).\n",
      "\n",
      "Cross-prompt t-tests (Gemma vs GPT-3.5) for all specified pairs:\n",
      "    model1    prompt1   model2           prompt2       metric   n1   n2  \\\n",
      "2    Gemma        CoT  GPT-3.5  chain_of_thought   complexity  391  585   \n",
      "4    Gemma        CoT  GPT-3.5  chain_of_thought      fluency  391  585   \n",
      "1    Gemma        CoT  GPT-3.5  chain_of_thought      grammar  391  585   \n",
      "0    Gemma        CoT  GPT-3.5  chain_of_thought      overall  391  585   \n",
      "3    Gemma        CoT  GPT-3.5  chain_of_thought  readability  391  585   \n",
      "..     ...        ...      ...               ...          ...  ...  ...   \n",
      "177  Gemma  Zero_Shot  GPT-3.5         zero_shot   complexity  309  585   \n",
      "179  Gemma  Zero_Shot  GPT-3.5         zero_shot      fluency  309  585   \n",
      "176  Gemma  Zero_Shot  GPT-3.5         zero_shot      grammar  309  585   \n",
      "175  Gemma  Zero_Shot  GPT-3.5         zero_shot      overall  309  585   \n",
      "178  Gemma  Zero_Shot  GPT-3.5         zero_shot  readability  309  585   \n",
      "\n",
      "         mean1        sd1      mean2        sd2    t_stat   p_value  cohens_d  \n",
      "2    91.543052  10.420247  92.461538  10.072969 -1.367459  0.171858 -0.089929  \n",
      "4    36.280694  40.314007  32.973697  34.644756  1.327234  0.184836  0.089332  \n",
      "1    91.132258  27.511699  94.518048  20.928433 -2.066464  0.039162 -0.142355  \n",
      "0    77.430745  16.626290  78.582774  13.160470 -1.150273  0.250423 -0.078653  \n",
      "3    77.065462  16.531545  78.442540  14.268962 -1.345816  0.178768 -0.090506  \n",
      "..         ...        ...        ...        ...       ...       ...       ...  \n",
      "177  90.679612  10.864765  93.230769  10.236405 -3.405680  0.000704 -0.243952  \n",
      "179  31.726471  38.439813  33.930845  33.973464 -0.848155  0.396712 -0.061957  \n",
      "176  90.193625  29.488780  97.628013  14.255384 -4.181126  0.000036 -0.357147  \n",
      "175  75.863108  17.447460  79.419721  11.157753 -3.249477  0.001244 -0.260350  \n",
      "178  76.522207  16.929640  74.680966  17.344908  1.533401  0.125671  0.107032  \n",
      "\n",
      "[180 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#############################\n",
    "# 1. Load and harmonize data\n",
    "#############################\n",
    "\n",
    "# Files and sheet names you specified\n",
    "gemma_raw = pd.read_excel(\"Gemma_Resultss.xlsx\", sheet_name=\"Gemma Results\")\n",
    "gpt_raw   = pd.read_excel(\"Results_GPTa.xlsx\",   sheet_name=\"Results\")\n",
    "\n",
    "print(\"Gemma columns:\", gemma_raw.columns.tolist())\n",
    "print(\"GPT columns:  \", gpt_raw.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 2. Select & rename relevant columns per model\n",
    "###############################################\n",
    "\n",
    "# Columns we care about from each file\n",
    "common_cols_gemma = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "common_cols_gpt = [\n",
    "    \"Prompting_strategy\",\n",
    "    \"Question_Type\",\n",
    "    \"Overall_Score\",\n",
    "    \"Grammar_Score\",\n",
    "    \"Complexity_Score\",\n",
    "    \"Readability_Score\",\n",
    "    \"Fluency_Score\",\n",
    "]\n",
    "\n",
    "# Subset safely (will raise if a column is missing → good sanity check)\n",
    "gemma = gemma_raw[common_cols_gemma].copy()\n",
    "gpt   = gpt_raw[common_cols_gpt].copy()\n",
    "\n",
    "# Add model labels\n",
    "gemma[\"model\"] = \"Gemma\"\n",
    "gpt[\"model\"]   = \"GPT-3.5\"\n",
    "\n",
    "# Normalize prompt column name\n",
    "gemma.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "gpt.rename(columns={\"Prompting_strategy\": \"prompt\"}, inplace=True)\n",
    "\n",
    "# Create lowercase metric aliases (easier to work with)\n",
    "rename_metrics = {\n",
    "    \"Overall_Score\":     \"overall\",\n",
    "    \"Grammar_Score\":     \"grammar\",\n",
    "    \"Complexity_Score\":  \"complexity\",\n",
    "    \"Readability_Score\": \"readability\",\n",
    "    \"Fluency_Score\":     \"fluency\",\n",
    "}\n",
    "\n",
    "for old, new in rename_metrics.items():\n",
    "    gemma[new] = gemma[old]\n",
    "    gpt[new]   = gpt[old]\n",
    "\n",
    "metric_cols = list(rename_metrics.values())\n",
    "\n",
    "#########################################\n",
    "# 3. Combine into a single long dataframe\n",
    "#########################################\n",
    "\n",
    "df = pd.concat([gemma, gpt], ignore_index=True)\n",
    "\n",
    "# Clean numeric types\n",
    "for c in metric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nCombined df head:\")\n",
    "print(df.head())\n",
    "\n",
    "##############################################\n",
    "# 4. Descriptive stats by model × prompt type\n",
    "##############################################\n",
    "\n",
    "group_cols = [\"model\", \"prompt\"]\n",
    "desc = (\n",
    "    df.groupby(group_cols)[metric_cols]\n",
    "      .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "desc.columns = [f\"{m}_{stat}\" for m, stat in desc.columns]\n",
    "desc = desc.reset_index()\n",
    "\n",
    "print(\"\\nDescriptive statistics by model × prompting strategy:\")\n",
    "print(desc)\n",
    "\n",
    "############################################\n",
    "# 5. Global independent t-tests (Gemma vs GPT)\n",
    "############################################\n",
    "\n",
    "def global_t_tests(df, metrics, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"Global (across all prompts) Welch t-tests for each metric.\"\"\"\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        a_vals = df.loc[df[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = df.loc[df[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        # Welch t-test\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"metric\":      metric,\n",
    "                \"n_gemma\":     n_a,\n",
    "                \"n_gpt\":       n_b,\n",
    "                \"mean_gemma\":  mean_a,\n",
    "                \"mean_gpt\":    mean_b,\n",
    "                \"t_stat\":      t_stat,\n",
    "                \"p_value\":     p_val,\n",
    "                \"cohens_d\":    cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "global_tests = global_t_tests(df, metrics=metric_cols)\n",
    "print(\"\\nGlobal t-tests (Gemma vs GPT-3.5) across ALL prompts:\")\n",
    "print(global_tests)\n",
    "\n",
    "##############################################################\n",
    "# 6. Per-prompt independent t-tests (same prompt name in both)\n",
    "##############################################################\n",
    "\n",
    "def independent_t_tests_by_prompt(df, metric, model_a=\"Gemma\", model_b=\"GPT-3.5\"):\n",
    "    \"\"\"\n",
    "    Welch t-test for each *prompt name* separately, for one metric,\n",
    "    assuming both models share the same prompt label.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for prompt in df[\"prompt\"].unique():\n",
    "        sub = df[df[\"prompt\"] == prompt]\n",
    "\n",
    "        a_vals = sub.loc[sub[\"model\"] == model_a, metric].dropna().values\n",
    "        b_vals = sub.loc[sub[\"model\"] == model_b, metric].dropna().values\n",
    "\n",
    "        # Need at least 2 per group for a meaningful test\n",
    "        if len(a_vals) < 2 or len(b_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(a_vals, b_vals, equal_var=False)\n",
    "\n",
    "        mean_a, mean_b = a_vals.mean(), b_vals.mean()\n",
    "        sd_a, sd_b = a_vals.std(ddof=1), b_vals.std(ddof=1)\n",
    "        n_a, n_b = len(a_vals), len(b_vals)\n",
    "\n",
    "        pooled_sd = np.sqrt(((n_a - 1)*sd_a**2 + (n_b - 1)*sd_b**2) / (n_a + n_b - 2))\n",
    "        cohens_d = (mean_a - mean_b) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"prompt\":      prompt,\n",
    "                \"metric\":      metric,\n",
    "                \"n_gemma\":     n_a,\n",
    "                \"n_gpt\":       n_b,\n",
    "                \"mean_gemma\":  mean_a,\n",
    "                \"mean_gpt\":    mean_b,\n",
    "                \"t_stat\":      t_stat,\n",
    "                \"p_value\":     p_val,\n",
    "                \"cohens_d\":    cohens_d,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_prompt_results_list = []\n",
    "for m in metric_cols:\n",
    "    res_m = independent_t_tests_by_prompt(df, metric=m)\n",
    "    print(f\"Metric {m}: {len(res_m)} per-prompt rows\")\n",
    "    if not res_m.empty:\n",
    "        per_prompt_results_list.append(res_m)\n",
    "\n",
    "if per_prompt_results_list:\n",
    "    per_prompt_tests = pd.concat(per_prompt_results_list, ignore_index=True)\n",
    "    print(\"\\nPer-prompt t-tests (Gemma vs GPT-3.5), same prompt name:\")\n",
    "    sort_cols = [c for c in [\"metric\", \"prompt\"] if c in per_prompt_tests.columns]\n",
    "    if sort_cols:\n",
    "        print(per_prompt_tests.sort_values(sort_cols))\n",
    "    else:\n",
    "        print(per_prompt_tests)\n",
    "else:\n",
    "    per_prompt_tests = pd.DataFrame()\n",
    "    print(\"\\nNo per-prompt t-tests could be computed \"\n",
    "          \"(likely < 2 items per model per prompt for each metric).\")\n",
    "\n",
    "###################################################################\n",
    "# 7. Cross-prompt specs: ALL pairs you want to check\n",
    "###################################################################\n",
    "\n",
    "cross_prompt_specs = [\n",
    "    # Gemma CoT vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+RC vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+RC\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+Seq vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma CoT+Seq_Rl vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"CoT+Seq_Rl\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma Few_Shot vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Few_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "\n",
    "    # Gemma Zero_Shot vs all GPT prompts\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_role_chain\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"chain_of_thought_plus_sequential_rl\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"few_shot\"},\n",
    "\n",
    "    {\"model1\": \"Gemma\",   \"prompt1\": \"Zero_Shot\",\n",
    "     \"model2\": \"GPT-3.5\", \"prompt2\": \"zero_shot\"},\n",
    "]\n",
    "\n",
    "###################################################################\n",
    "# 8. Run cross-prompt t-tests for ALL metrics over these pairs\n",
    "###################################################################\n",
    "\n",
    "def run_cross_prompt_tests(df, metrics, specs):\n",
    "    \"\"\"\n",
    "    For each spec dict (model1, prompt1, model2, prompt2) and each metric,\n",
    "    run a Welch t-test and compute Cohen's d, including sd1 and sd2.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for spec in specs:\n",
    "        m1 = spec[\"model1\"]\n",
    "        p1 = spec[\"prompt1\"]\n",
    "        m2 = spec[\"model2\"]\n",
    "        p2 = spec[\"prompt2\"]\n",
    "\n",
    "        for metric in metrics:\n",
    "            v1 = df.loc[(df[\"model\"] == m1) & (df[\"prompt\"] == p1), metric].dropna().values\n",
    "            v2 = df.loc[(df[\"model\"] == m2) & (df[\"prompt\"] == p2), metric].dropna().values\n",
    "\n",
    "            if len(v1) < 2 or len(v2) < 2:\n",
    "                # Not enough data for a meaningful test\n",
    "                continue\n",
    "\n",
    "            t_stat, p_val = stats.ttest_ind(v1, v2, equal_var=False)\n",
    "\n",
    "            mean1, mean2 = v1.mean(), v2.mean()\n",
    "            sd1, sd2 = v1.std(ddof=1), v2.std(ddof=1)\n",
    "            n1, n2 = len(v1), len(v2)\n",
    "\n",
    "            pooled_sd = np.sqrt(((n1 - 1)*sd1**2 + (n2 - 1)*sd2**2) / (n1 + n2 - 2))\n",
    "            cohens_d = (mean1 - mean2) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"model1\":   m1,\n",
    "                    \"prompt1\":  p1,\n",
    "                    \"model2\":   m2,\n",
    "                    \"prompt2\":  p2,\n",
    "                    \"metric\":   metric,\n",
    "                    \"n1\":       n1,\n",
    "                    \"n2\":       n2,\n",
    "                    \"mean1\":    mean1,\n",
    "                    \"sd1\":      sd1,\n",
    "                    \"mean2\":    mean2,\n",
    "                    \"sd2\":      sd2,\n",
    "                    \"t_stat\":   t_stat,\n",
    "                    \"p_value\":  p_val,\n",
    "                    \"cohens_d\": cohens_d,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cross_prompt_tests = run_cross_prompt_tests(df, metrics=metric_cols, specs=cross_prompt_specs)\n",
    "\n",
    "print(\"\\nCross-prompt t-tests (Gemma vs GPT-3.5) for all specified pairs:\")\n",
    "if not cross_prompt_tests.empty:\n",
    "    print(cross_prompt_tests.sort_values([\"prompt1\", \"prompt2\", \"metric\"]))\n",
    "else:\n",
    "    print(\"No cross-prompt tests could be computed (check labels / sample sizes).\")\n",
    "\n",
    "###########################################\n",
    "# 9. Save outputs (optional)\n",
    "###########################################\n",
    "\n",
    "desc.to_csv(\"adesc_by_model_prompt.csv\", index=False)\n",
    "global_tests.to_csv(\"AGlobal_t_tests_gemma_vs_gpt.csv\", index=False)\n",
    "per_prompt_tests.to_csv(\"APer_prompt_t_tests_gemma_vs_gpt_same_prompt.csv\", index=False)\n",
    "cross_prompt_tests.to_csv(\"ACross_prompt_t_tests_gemma_vs_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88b7d81f-9303-4904-8dc9-17e2c917ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading: Results_GPTa.xlsx with header=0\n",
      "Columns (header=0): ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "✅ Found required columns with header=0\n",
      "\n",
      "Final GPT columns: ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Computing Jaccard for GPT (Question + Correct_Answer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing max Jaccard within file: 100%|███| 3509/3509 [00:09<00:00, 378.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved GPT Jaccard results to: results_GPT_with_jaccard.xlsx\n",
      "\n",
      "GPT max Jaccard summary:\n",
      "count    3509.000000\n",
      "mean        0.838467\n",
      "std         0.178802\n",
      "min         0.256410\n",
      "25%         0.714286\n",
      "50%         0.882353\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: max_jaccard_within, dtype: float64\n",
      "\n",
      "Reading: Gemma_Resultss.xlsx with header=0\n",
      "Columns (header=0): ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Required columns not found with header=0. Trying header=1...\n",
      "Columns (header=1): ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Could not find required columns {'Choice_3', 'Choice_2', 'Choice_1', 'Question'} in Gemma_Resultss.xlsx.\\nheader=0 columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\\nheader=1 columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 151\u001b[0m\n\u001b[1;32m    144\u001b[0m gemma_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemma_Resultss.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# adjust if needed\u001b[39;00m\n\u001b[1;32m    149\u001b[0m required_gemma_cols \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_3\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 151\u001b[0m df_gemma \u001b[38;5;241m=\u001b[39m read_excel_with_required_cols(gemma_path, required_gemma_cols)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Gemma columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(df_gemma\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m    155\u001b[0m gemma_text_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoice_3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[66], line 105\u001b[0m, in \u001b[0;36mread_excel_with_required_cols\u001b[0;34m(path, required_cols)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Found required columns with header=1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df1\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find required columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader=0 columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df0\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader=1 columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df1\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Could not find required columns {'Choice_3', 'Choice_2', 'Choice_1', 'Question'} in Gemma_Resultss.xlsx.\\nheader=0 columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\\nheader=1 columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: text normalization & Jaccard\n",
    "# ----------------------------\n",
    "\n",
    "def normalize_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove punctuation, and split into tokens.\n",
    "    Returns a set of tokens for Jaccard similarity.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    # Keep only letters, digits, and spaces\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return set()\n",
    "    return set(text.split())\n",
    "\n",
    "def jaccard_similarity(set_a, set_b):\n",
    "    if not set_a and not set_b:\n",
    "        return 0.0\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "def compute_within_file_max_jaccard(df, text_cols, id_col_name=None):\n",
    "    \"\"\"\n",
    "    For each row:\n",
    "      - Concatenate text_cols into a single string\n",
    "      - Tokenize\n",
    "      - Compute max Jaccard vs all other rows in the same file\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Combine selected text columns\n",
    "    df[\"combined_text\"] = (\n",
    "        df[text_cols]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Tokenize all rows once\n",
    "    token_sets = [normalize_and_tokenize(t) for t in df[\"combined_text\"]]\n",
    "\n",
    "    n = len(df)\n",
    "    max_jaccard = np.zeros(n, dtype=float)\n",
    "\n",
    "    for i in tqdm(range(n), desc=\"Computing max Jaccard within file\"):\n",
    "        s_i = token_sets[i]\n",
    "        best = 0.0\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            s_j = token_sets[j]\n",
    "            sim = jaccard_similarity(s_i, s_j)\n",
    "            if sim > best:\n",
    "                best = sim\n",
    "        max_jaccard[i] = best\n",
    "\n",
    "    df[\"max_jaccard_within\"] = max_jaccard\n",
    "\n",
    "    if id_col_name is not None and id_col_name not in df.columns:\n",
    "        df[id_col_name] = np.arange(1, n + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Robust Excel reader (handles extra header row)\n",
    "# ----------------------------\n",
    "\n",
    "def read_excel_with_required_cols(path, required_cols):\n",
    "    \"\"\"\n",
    "    Try reading Excel with default header=0.\n",
    "    If required columns are missing, try header=1 (skip first row),\n",
    "    which often fixes 'Unnamed: 0' header issues.\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading: {path} with header=0\")\n",
    "    df0 = pd.read_excel(path, header=0)\n",
    "    df0.columns = df0.columns.map(lambda x: str(x).strip())\n",
    "    print(\"Columns (header=0):\", list(df0.columns))\n",
    "\n",
    "    if required_cols.issubset(set(df0.columns)):\n",
    "        print(\"✅ Found required columns with header=0\")\n",
    "        return df0\n",
    "\n",
    "    print(\"Required columns not found with header=0. Trying header=1...\")\n",
    "    df1 = pd.read_excel(path, header=1)\n",
    "    df1.columns = df1.columns.map(lambda x: str(x).strip())\n",
    "    print(\"Columns (header=1):\", list(df1.columns))\n",
    "\n",
    "    if required_cols.issubset(set(df1.columns)):\n",
    "        print(\"✅ Found required columns with header=1\")\n",
    "        return df1\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Could not find required columns {required_cols} in {path}.\\n\"\n",
    "        f\"header=0 columns: {list(df0.columns)}\\n\"\n",
    "        f\"header=1 columns: {list(df1.columns)}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# GPT: Question + Correct_Answer\n",
    "# ----------------------------\n",
    "\n",
    "# You can change this to your actual filename if different\n",
    "gpt_path = \"Results_GPTa.xlsx\"\n",
    "\n",
    "required_gpt_cols = {\"Question\", \"Correct_Answer\"}\n",
    "\n",
    "df_gpt = read_excel_with_required_cols(gpt_path, required_gpt_cols)\n",
    "\n",
    "print(\"\\nFinal GPT columns:\", list(df_gpt.columns))\n",
    "\n",
    "gpt_text_cols = [\"Question\", \"Correct_Answer\"]\n",
    "\n",
    "print(\"\\nComputing Jaccard for GPT (Question + Correct_Answer)...\")\n",
    "df_gpt_jacc = compute_within_file_max_jaccard(\n",
    "    df_gpt,\n",
    "    text_cols=gpt_text_cols,\n",
    "    id_col_name=\"Item_ID\"\n",
    ")\n",
    "\n",
    "gpt_out_path = \"results_GPT_with_jaccard.xlsx\"\n",
    "df_gpt_jacc.to_excel(gpt_out_path, index=False)\n",
    "print(f\"\\nSaved GPT Jaccard results to: {gpt_out_path}\")\n",
    "\n",
    "print(\"\\nGPT max Jaccard summary:\")\n",
    "print(df_gpt_jacc[\"max_jaccard_within\"].describe())\n",
    "\n",
    "# ----------------------------\n",
    "# Gemma: Question + Choice_1 + Choice_2 + Choice_3\n",
    "# ----------------------------\n",
    "\n",
    "gemma_path = \"Gemma_Resultss.xlsx\"  # adjust if needed\n",
    "\n",
    "\n",
    "required_gemma_cols = {\"Question\", \"Correct_Answer\"}\n",
    "\n",
    "df_gemma = read_excel_with_required_cols(gemma_path, required_gemma_cols)\n",
    "\n",
    "print(\"\\nFinal Gemma columns:\", list(df_gemma.columns))\n",
    "\n",
    "gemma_text_cols = [\"Question\", \"Choice_1\", \"Choice_2\", \"Choice_3\"]\n",
    "\n",
    "print(\"\\nComputing Jaccard for Gemma (Question + 3 choices)...\")\n",
    "df_gemma_jacc = compute_within_file_max_jaccard(\n",
    "    df_gemma,\n",
    "    text_cols=gemma_text_cols,\n",
    "    id_col_name=\"Item_ID\"\n",
    ")\n",
    "\n",
    "gemma_out_path = \"Gemma_Results_with_jaccard.xlsx\"\n",
    "df_gemma_jacc.to_excel(gemma_out_path, index=False)\n",
    "print(f\"\\nSaved Gemma Jaccard results to: {gemma_out_path}\")\n",
    "\n",
    "print(\"\\nGemma max Jaccard summary:\")\n",
    "print(df_gemma_jacc[\"max_jaccard_within\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b19b34e-7bf6-4db3-a6d2-e352a8dbf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading: Results_GPTa.xlsx with header=0\n",
      "Columns (header=0): ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "✅ Found required columns with header=0\n",
      "\n",
      "Final GPT columns: ['Row', 'Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count', 'Errors']\n",
      "\n",
      "Computing Jaccard for GPT (Question + Correct_Answer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing max Jaccard within file: 100%|███| 3509/3509 [00:09<00:00, 378.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved GPT Jaccard results to: results_GPT_with_jaccard.xlsx\n",
      "\n",
      "GPT max Jaccard summary:\n",
      "count    3509.000000\n",
      "mean        0.838467\n",
      "std         0.178802\n",
      "min         0.256410\n",
      "25%         0.714286\n",
      "50%         0.882353\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: max_jaccard_within, dtype: float64\n",
      "\n",
      "Reading: Gemma_modified.xlsx with header=0\n",
      "Columns (header=0): ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count']\n",
      "✅ Found required columns with header=0\n",
      "\n",
      "Final Gemma columns: ['Prompting_strategy', 'Question_Type', 'Question', 'Correct_Answer', 'Choice_1', 'Choice_2', 'Choice_3', 'Word_Difficulty', 'Task_Difficulty', 'Text', 'Overall_Score', 'Grammar_Score', 'Complexity_Score', 'Readability_Score', 'Fluency_Score', 'Error_Count']\n",
      "\n",
      "Computing Jaccard for Gemma (Question + 3 choices)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing max Jaccard within file: 100%|███| 2143/2143 [00:03<00:00, 638.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved Gemma Jaccard results to: Gemma_Results_with_jaccard.xlsx\n",
      "\n",
      "Gemma max Jaccard summary:\n",
      "count    2143.000000\n",
      "mean        0.969555\n",
      "std         0.123335\n",
      "min         0.208333\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: max_jaccard_within, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: text normalization & Jaccard\n",
    "# ----------------------------\n",
    "\n",
    "def normalize_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove punctuation, and split into tokens.\n",
    "    Returns a set of tokens for Jaccard similarity.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    # Keep only letters, digits, and spaces\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return set()\n",
    "    return set(text.split())\n",
    "\n",
    "def jaccard_similarity(set_a, set_b):\n",
    "    if not set_a and not set_b:\n",
    "        return 0.0\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "def compute_within_file_max_jaccard(df, text_cols, id_col_name=None):\n",
    "    \"\"\"\n",
    "    For each row:\n",
    "      - Concatenate text_cols into a single string\n",
    "      - Tokenize\n",
    "      - Compute max Jaccard vs all other rows in the same file\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Combine selected text columns\n",
    "    df[\"combined_text\"] = (\n",
    "        df[text_cols]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Tokenize all rows once\n",
    "    token_sets = [normalize_and_tokenize(t) for t in df[\"combined_text\"]]\n",
    "\n",
    "    n = len(df)\n",
    "    max_jaccard = np.zeros(n, dtype=float)\n",
    "\n",
    "    for i in tqdm(range(n), desc=\"Computing max Jaccard within file\"):\n",
    "        s_i = token_sets[i]\n",
    "        best = 0.0\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            s_j = token_sets[j]\n",
    "            sim = jaccard_similarity(s_i, s_j)\n",
    "            if sim > best:\n",
    "                best = sim\n",
    "        max_jaccard[i] = best\n",
    "\n",
    "    df[\"max_jaccard_within\"] = max_jaccard\n",
    "\n",
    "    if id_col_name is not None and id_col_name not in df.columns:\n",
    "        df[id_col_name] = np.arange(1, n + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Robust Excel reader (handles extra / shifted header rows)\n",
    "# ----------------------------\n",
    "\n",
    "def read_excel_with_required_cols(path, required_cols):\n",
    "    \"\"\"\n",
    "    Try to read Excel and ensure required_cols exist as columns.\n",
    "    Steps:\n",
    "      1) Try header=0\n",
    "      2) Try header=1\n",
    "      3) Fallback: header=None and search for the row that contains all required_cols\n",
    "    \"\"\"\n",
    "    # --- Try header=0 ---\n",
    "    print(f\"\\nReading: {path} with header=0\")\n",
    "    df0 = pd.read_excel(path, header=0)\n",
    "    df0.columns = df0.columns.map(lambda x: str(x).strip())\n",
    "    print(\"Columns (header=0):\", list(df0.columns))\n",
    "\n",
    "    if required_cols.issubset(set(df0.columns)):\n",
    "        print(\"✅ Found required columns with header=0\")\n",
    "        return df0\n",
    "\n",
    "    # --- Try header=1 ---\n",
    "    print(\"Required columns not found with header=0. Trying header=1...\")\n",
    "    df1 = pd.read_excel(path, header=1)\n",
    "    df1.columns = df1.columns.map(lambda x: str(x).strip())\n",
    "    print(\"Columns (header=1):\", list(df1.columns))\n",
    "\n",
    "    if required_cols.issubset(set(df1.columns)):\n",
    "        print(\"✅ Found required columns with header=1\")\n",
    "        return df1\n",
    "\n",
    "    # --- Fallback: header=None & scan rows for a header row containing required cols ---\n",
    "    print(\"Required columns not found with header=0 or header=1.\")\n",
    "    print(\"Trying header=None and scanning for a row that contains all required columns...\")\n",
    "\n",
    "    df_raw = pd.read_excel(path, header=None)\n",
    "    n_rows, n_cols = df_raw.shape\n",
    "    print(f\"Raw shape (header=None): {df_raw.shape}\")\n",
    "\n",
    "    header_row_idx = None\n",
    "    for i in range(n_rows):\n",
    "        row_vals = [str(x).strip() for x in df_raw.iloc[i].tolist()]\n",
    "        row_set = set(row_vals)\n",
    "        if required_cols.issubset(row_set):\n",
    "            header_row_idx = i\n",
    "            print(f\"✅ Found header row at index {i}: {row_vals}\")\n",
    "            break\n",
    "\n",
    "    if header_row_idx is None:\n",
    "        raise KeyError(\n",
    "            f\"Could not find required columns {required_cols} in any row of {path}.\\n\"\n",
    "            f\"Example of first few rows (header=None):\\n{df_raw.head(5)}\"\n",
    "        )\n",
    "\n",
    "    # Build DataFrame using that row as header\n",
    "    header_vals = [str(x).strip() for x in df_raw.iloc[header_row_idx].tolist()]\n",
    "    df2 = df_raw.iloc[header_row_idx + 1 :].copy()\n",
    "    df2.columns = header_vals\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    print(\"Columns (from detected header row):\", list(df2.columns))\n",
    "\n",
    "    if not required_cols.issubset(set(df2.columns)):\n",
    "        raise KeyError(\n",
    "            f\"Even after detecting header row at {header_row_idx}, \"\n",
    "            f\"the DataFrame does not contain required columns {required_cols}. \"\n",
    "            f\"Final columns: {list(df2.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(\"✅ Successfully constructed DataFrame with required columns from detected header row.\")\n",
    "    return df2\n",
    "\n",
    "# ----------------------------\n",
    "# GPT: Question + Correct_Answer\n",
    "# ----------------------------\n",
    "\n",
    "gpt_path = \"Results_GPTa.xlsx\"  # your GPT file\n",
    "\n",
    "required_gpt_cols = {\"Question\", \"Correct_Answer\"}\n",
    "\n",
    "df_gpt = read_excel_with_required_cols(gpt_path, required_gpt_cols)\n",
    "\n",
    "print(\"\\nFinal GPT columns:\", list(df_gpt.columns))\n",
    "\n",
    "gpt_text_cols = [\"Question\", \"Correct_Answer\"]\n",
    "\n",
    "print(\"\\nComputing Jaccard for GPT (Question + Correct_Answer)...\")\n",
    "df_gpt_jacc = compute_within_file_max_jaccard(\n",
    "    df_gpt,\n",
    "    text_cols=gpt_text_cols,\n",
    "    id_col_name=\"Item_ID\"\n",
    ")\n",
    "\n",
    "gpt_out_path = \"results_GPT_with_jaccard.xlsx\"\n",
    "df_gpt_jacc.to_excel(gpt_out_path, index=False)\n",
    "print(f\"\\nSaved GPT Jaccard results to: {gpt_out_path}\")\n",
    "\n",
    "print(\"\\nGPT max Jaccard summary:\")\n",
    "print(df_gpt_jacc[\"max_jaccard_within\"].describe())\n",
    "\n",
    "# ----------------------------\n",
    "# Gemma: Question + Choice_1 + Choice_2 + Choice_3\n",
    "# ----------------------------\n",
    "\n",
    "gemma_path = \"Gemma_modified.xlsx\"  # adjust if needed\n",
    "\n",
    "required_gemma_cols = {\"Question\", \"Choice_1\", \"Choice_2\", \"Choice_3\"}\n",
    "\n",
    "df_gemma = read_excel_with_required_cols(gemma_path, required_gemma_cols)\n",
    "\n",
    "print(\"\\nFinal Gemma columns:\", list(df_gemma.columns))\n",
    "\n",
    "gemma_text_cols = [\"Question\", \"Choice_1\", \"Choice_2\", \"Choice_3\"]\n",
    "\n",
    "print(\"\\nComputing Jaccard for Gemma (Question + 3 choices)...\")\n",
    "df_gemma_jacc = compute_within_file_max_jaccard(\n",
    "    df_gemma,\n",
    "    text_cols=gemma_text_cols,\n",
    "    id_col_name=\"Item_ID\"\n",
    ")\n",
    "\n",
    "gemma_out_path = \"Gemma_Results_with_jaccard.xlsx\"\n",
    "df_gemma_jacc.to_excel(gemma_out_path, index=False)\n",
    "print(f\"\\nSaved Gemma Jaccard results to: {gemma_out_path}\")\n",
    "\n",
    "print(\"\\nGemma max Jaccard summary:\")\n",
    "print(df_gemma_jacc[\"max_jaccard_within\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5878a822-ae5d-47a5-9316-19f0b62d6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT: Exact full-item overlap ===\n",
      "Total items:     3509\n",
      "Unique items:    2613 (74.47%)\n",
      "Exact duplicates:896 (25.53%)\n",
      "\n",
      "=== Gemma: Exact full-item overlap ===\n",
      "Total items:     2143\n",
      "Unique items:    393 (18.34%)\n",
      "Exact duplicates:1750 (81.66%)\n",
      "\n",
      "Saved updated files with 'full_item' and 'is_exact_duplicate' columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load the Jaccard-annotated files (or the original ones if you prefer)\n",
    "# -------------------------------------------------------------------\n",
    "gpt_path   = \"results_GPT_with_jaccard.xlsx\"      # or your original GPT file\n",
    "gemma_path = \"Gemma_Results_with_jaccard.xlsx\"    # or Gemma_modified.xlsx\n",
    "\n",
    "df_gpt   = pd.read_excel(gpt_path)\n",
    "df_gemma = pd.read_excel(gemma_path)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build \"full item\" strings:\n",
    "#   - GPT:   Question + Correct_Answer\n",
    "#   - Gemma: Question + Choice_1 + Choice_2 + Choice_3\n",
    "# -------------------------------------------------------------------\n",
    "def build_gpt_full_item(row):\n",
    "    q  = str(row.get(\"Question\", \"\") or \"\").strip()\n",
    "    ca = str(row.get(\"Correct_Answer\", \"\") or \"\").strip()\n",
    "    return (q + \" [ANS] \" + ca).strip()\n",
    "\n",
    "def build_gemma_full_item(row):\n",
    "    q  = str(row.get(\"Question\", \"\") or \"\").strip()\n",
    "    c1 = str(row.get(\"Choice_1\", \"\") or \"\").strip()\n",
    "    c2 = str(row.get(\"Choice_2\", \"\") or \"\").strip()\n",
    "    c3 = str(row.get(\"Choice_3\", \"\") or \"\").strip()\n",
    "    return (q + \" [A] \" + c1 + \" [B] \" + c2 + \" [C] \" + c3).strip()\n",
    "\n",
    "df_gpt[\"full_item\"]   = df_gpt.apply(build_gpt_full_item, axis=1)\n",
    "df_gemma[\"full_item\"] = df_gemma.apply(build_gemma_full_item, axis=1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Exact-duplicate analysis\n",
    "# -------------------------------------------------------------------\n",
    "def summarize_exact_duplicates(df, label):\n",
    "    n_total   = len(df)\n",
    "    n_unique  = df[\"full_item\"].nunique()\n",
    "    n_dupes   = n_total - n_unique\n",
    "    pct_unique = n_unique / n_total * 100\n",
    "    pct_dupes  = n_dupes / n_total * 100\n",
    "\n",
    "    print(f\"\\n=== {label}: Exact full-item overlap ===\")\n",
    "    print(f\"Total items:     {n_total}\")\n",
    "    print(f\"Unique items:    {n_unique} ({pct_unique:.2f}%)\")\n",
    "    print(f\"Exact duplicates:{n_dupes} ({pct_dupes:.2f}%)\")\n",
    "\n",
    "    # Mark which rows are duplicates (beyond the first occurrence)\n",
    "    df[\"is_exact_duplicate\"] = df.duplicated(\"full_item\", keep=\"first\")\n",
    "    return df\n",
    "\n",
    "df_gpt   = summarize_exact_duplicates(df_gpt,   \"GPT\")\n",
    "df_gemma = summarize_exact_duplicates(df_gemma, \"Gemma\")\n",
    "\n",
    "# Save back if you want the flags\n",
    "df_gpt.to_excel(\"results_GPT_with_jaccard_and_exact.xlsx\", index=False)\n",
    "df_gemma.to_excel(\"Gemma_Results_with_jaccard_and_exact.xlsx\", index=False)\n",
    "print(\"\\nSaved updated files with 'full_item' and 'is_exact_duplicate' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399858a9-f049-465c-a913-3c4a9e70027e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
